% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CausalImage_Heterogeneity.R
\name{AnalyzeImageHeterogeneity}
\alias{AnalyzeImageHeterogeneity}
\title{Decompose treatment effect heterogeneity by image}
\usage{
AnalyzeImageHeterogeneity(obsW, obsY, imageKeysOfUnits, acquireImageFxn, kClust_est, ...)
}
\arguments{
\item{obsW}{A numeric vector where \code{0}'s correspond to control units and \code{1}'s to treated units.}

\item{obsY}{A numeric vector containing observed outcomes.}

\item{X}{(optional) A numeric matrix containing tabular information used if \code{orthogonalize = T}.}

\item{orthogonalize}{(default = \code{F}) A Boolean specifying whether to perform the image decomposition after orthogonalizing with respect to tabular covariates specified in \code{X}.}

\item{imageKeysOfUnits}{(default = \code{1:length(obsY)}) A vector of length \code{length(obsY)} specifying the unique image ID associated with each unit. Samples of \code{imageKeysOfUnits} are fed into \code{acquireImageFxn} to call images into memory.}

\item{kClust_est}{(default = \code{2L}) Integer specifying the number of clusters used in estimation.}

\item{acquireImageFxn}{A function specifying how to load images representations associated with \code{imageKeysOfUnits} into memory. For example, if observation \code{3} has a value  of \code{"a34f"} in \code{imageKeysOfUnits}, \code{acquireImageFxn} should extract the image associated with the unique key \code{"a34f"}.
First argument should be image key values and second argument have be \code{training} (in case of behavior change in training/inference).}

\item{file}{(default = \code{NULL}) Path to a tfrecord file generated by \code{WriteTfRecord}.}

\item{transportabilityMat}{(optional) A matrix with a column named \code{key} specifying keys to be used by \code{acquireImageFxn} for generating treatment effect predictions for out-of-sample points in earth observation data settings.}

\item{long, lat}{(optional) Vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.}

\item{conda_env}{(default = \code{NULL}) A string specifying a conda environment wherein \code{tensorflow}, \code{tensorflow_probability}, and \code{gc} are installed.}

\item{conda_env_required}{(default = \code{F}) A Boolean stating whether use of the specified conda environment is required.}

\item{figuresTag}{(default = \code{""}) A string specifying an identifier that is appended to all figure names.}

\item{figuresPath}{(default = \code{"./"}) A string specifying file path for saved figures made in the analysis.}

\item{plotBands}{(default = \code{1L}) An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, \code{plotBands} should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RGB plotting).}

\item{simMode}{(default = \code{F}) Should the analysis be performed in comparison with ground truth from simulation?}

\item{plotResults}{(default = \code{T}) Should analysis results be plotted?}

\item{nDepthHidden_conv}{(default = \code{3L}) Hidden depth of convolutional layer.}

\item{nDepthHidden_dense}{(default = \code{0L}) Hidden depth of dense layers. Default of \code{0L} means a single projection layer is performed after the convolutional layer (i.e., no hidden layers are used).}

\item{maxPoolSize}{(default = \code{2L}) Integer specifying the max pooling size used in the convolutional layers.}

\item{strides}{(default = \code{2L}) Integer specifying the strides used in the convolutional layers.=}

\item{yDensity}{(default = \code{normal}) Specifies the density for the outcome. Current options include \code{normal} and \code{lognormal}.}

\item{nMonte_variational}{(default = \code{5L}) An integer specifying how many Monte Carlo iterations to use in the
calculation of the expected likelihood in each training step.}

\item{nMonte_predictive}{(default = \code{20L}) An integer specifying how many Monte Carlo iterations to use in the calculation
of posterior means (e.g., mean cluster probabilities).}

\item{nMonte_salience}{(default = \code{100L}) An integer specifying how many Monte Carlo iterations to use in the calculation
of the salience maps (e.g., image gradients of expected cluster probabilities).}

\item{batchSize}{(default = \code{25L}) Batch size used in SGD optimization.}

\item{kernelSize}{(default = \code{5L}) Dimensions used in convolution kernels.}

\item{nSGD}{(default = \code{400L}) Number of stochastic gradient descent (SGD) iterations.}

\item{nDenseWidth}{(default = \code{32L}) Width of dense projection layers post-convolutions.}

\item{reparameterizationType}{(default = \code{"Flipout"}) Either \code{"Flipout"}, or \code{"Reparameterization"}. Specifies the estimator used in the Bayesian neural components. With \code{"Flipout"}, convolutions are performed via CPU; with `"Reparameterization", they are performed by GPU if available.}

\item{doConvLowerDimProj}{(default = \code{T}) Should we project the \code{nFilters} convolutional feature dimensions down to \code{nDimLowerDimConv} to reduce the number of required parameters.}

\item{nDimLowerDimConv}{(default = \code{3L}) If \code{doConvLowerDimProj = T}, then, in each convolutional layer, we project the \code{nFilters} feature dimensions down to \code{nDimLowerDimConv} to reduce the number of parameters needed.}

\item{nFilters}{(default = \code{32L}) Integer specifying the number of convolutional filters used.}

\item{channelNormalize}{(default = \code{T}) Should channelwise image feature normalization be attempted? Default is \code{T}, as this improves training.}

\item{modelClass}{(default = \code{"cnn"}) Either \code{"cnn"} or \code{"embeddings"}.}

\item{nEmbedDim}{(default = \code{96L}) Integer specifying the image/image sequence embedding dimension. Used if \code{modelClass = "embeddings"}.}

\item{TfRecords_BufferScaler}{(default = \code{4L}) The buffer size used in \code{tfrecords} mode is \code{batchSize*TfRecords_BufferScaler}. Lower \code{TfRecords_BufferScaler} towards 1 if out-of-memory problems.}

\item{quiet}{(default = \code{F}) Should we suppress information about progress?}
}
\value{
Returns a list consiting of \itemize{
\item \code{clusterTaus_mean} default
\item \code{clusterTaus_sd} Estimated image effect cluster standard deviations.
\item \code{clusterProbs_mean}. Estimated mean image effect cluster probabilities.
\item \code{clusterTaus_sd}. Estimated image effect cluster probability standard deviations.
\item \code{clusterProbs_lowerConf}. Estimated lower confidence for effect cluster probabilities.
\item \code{impliedATE}. Implied ATE.
\item \code{individualTau_est}. Estimated individual-level image-based treatment effects.
\item \code{transportabilityMat}. Transportability matrix withestimated cluster information.
\item \code{plottedCoordinates}. List containing coordinates plotted in salience maps.
\item \code{whichNA_dropped}. A vector containing observations dropped due to missingness.
}
}
\description{
Implements the image heterogeneity decomposition analysis of Jerzak, Johansson, and Daoud (2023). Users
input in treatment and outcome data, along with a function specifying how to load in images
using keys referenced to each unit (since loading in all image data will usually not be possible due to memory limitations).
This function by default performs estimation, constructs salience maps, and can optionally perform
estimation for new areas outside the original study sites in a transportability analysis.
}
\section{References}{

\itemize{
\item Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Image-based Treatment Effect Heterogeneity. Forthcoming in \emph{Proceedings of the Second Conference on Causal Learning and Reasoning (CLeaR), Proceedings of Machine Learning Research (PMLR)}, 2023.
}
}

\examples{
# For a tutorial, see
# github.com/cjerzak/causalimages-software/

}
