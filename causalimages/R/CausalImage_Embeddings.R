#!/usr/bin/env Rscript
#' Generates image and video embeddings useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' Generates image and video embeddings useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' @usage
#'
#' GetImageEmbeddings(imageKeysOfUnits, acquireImageFxn, nEmbedDim, ...)
#'
#' @param acquireImageFxn A function specifying how to load images representations associated with `imageKeysOfUnits` into memory. For example, if observation `3` has a value  of `"a34f"` in `imageKeysOfUnits`, `acquireImageFxn` should extract the image associated with the unique key `"a34f"`.
#' First argument should be image key values and second argument have be `training` (in case different behavior in training/inference mode).
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into `acquireImageFxn` to call images into memory.
#' @param conda_env (default = `NULL`) A string specifying a conda environment wherein `tensorflow`, `tensorflow_probability`, and `gc` are installed.
#' @param conda_env_required (default = `F`) A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize (default = `5L`) Dimensions used in the convolution kernels.
#' @param temporalKernelSize (default = `2L`) Dimensions used in the temporal part of the convolution kernels if using image sequences.
#' @param nEmbedDim (default = `128L`) Number of embedding features output.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.
#' @param batchSize (default = `50L`) Integer specifying batch size in obtaining embeddings.
#' @param file (default = `NULL`) Path to a tfrecord file generated by `WriteTfRecord`.
#' @param TfRecords_BufferScaler (default = `10L`) The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param seed (default = `NULL`) Integer specifying the seed.
#' @param outputType (default = `"R"`) Either `"R"` or `"tensorflow"` indicating whether to output R or tensorflow arrays.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `embeddings` (matrix) A matrix containingimage/video embeddings, with rows corresponding to observations.
#' \item `embeddings_fxn` (function) The functioning performing the embedding, returned for re-use.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

GetImageEmbeddings <- function(
    acquireImageFxn  = NULL,
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = NULL,
    conda_env_required = F,

    nEmbedDim = 128L,
    batchSize = 50L,
    strides = 1L,
    temporalKernelSize = 2L,
    kernelSize = 3L,
    TfRecords_BufferScaler = 10L,
    dataType = dataType,
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    seed = NULL,
    quiet = F){

  if(   "try-error" %in% class(try(tf$constant(1.),T))   ){
    print("Initializing the tensorflow environment...")
    print("Looking for Python modules tensorflow, gc...")
    library(tensorflow); library(keras)
    if(!is.null(conda_env)){
      try(tensorflow::use_condaenv(conda_env, required = conda_env_required),T)
    }
    Sys.sleep(1.); try(tf$square(1.),T); Sys.sleep(1.);
    try(tf$config$experimental$set_memory_growth(tf$config$list_physical_devices('GPU')[[1]],T),T)
    try( tf$config$set_soft_device_placement( T ) , T)

    try(tf$random$set_seed(  c( ifelse(is.null(tf_seed),
                                       yes = 123431L, no = as.integer(tf_seed)  ) )), T)
    try(tf$keras$utils$set_random_seed( c( ifelse(is.null(tf_seed),
                                                  yes = 123419L, no = as.integer(tf_seed)  ) )), T)

    # import python garbage collectors
    py_gc <- reticulate::import("gc")
  }
  gc(); try(py_gc$collect(), T)

  if(batchSize > length(imageKeysOfUnits)){
    batchSize <- length( imageKeysOfUnits  )
  }

  acquireImageMethod <- "functional";
  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    acquireImageMethod <- "tf_record"

    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/")
    setwd( new_wd )
    tf_dataset = tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- dataType == "video"
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
      dataset <- dataset$shuffle(tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )

    # checks
    # ds_iterator_inference$output_shapes; ds_iterator_train$output_shapes
    # ds_next_train <- reticulate::iter_next( ds_iterator_train )
    # ds_next_inference <- reticulate::iter_next( ds_iterator_inference )
  }

  # coerce output to tf$constant
  if(acquireImageMethod == "functional"){
    myType <- acquireImageFxn(imageKeysOfUnits[1:2], training = F)
    environment(acquireImageFxn) <- environment()
    test_ <- acquireImageFxn(imageKeysOfUnits[1:5],training = F)
    if(!"tensorflow.tensor" %in% class(test_)){
      acquireImageFxn_as_input <- acquireImageFxn
      acquireImageFxn <- function(keys, training){
        m_ <- tf$constant(acquireImageFxn_as_input(keys, training),tf$float32)
        if(length(m_$shape) == 3){
          # expand across batch dimension if receiving no batch dimension
          m_ <- tf$expand_dims(m_,0L)
        }
        return( m_ )
      }
    }
  }
  if(acquireImageMethod == "tf_record"){
    setwd(orig_wd)
    test_ <- tf$expand_dims(GetElementFromTfRecordAtIndices( indices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             nObs = length(imageKeysOfUnits))[[1]],0L)
    setwd(new_wd)
  }

  imageDims <- length( dim(test_) ) - 2L
  if(nEmbedDim %% 2 == 0){ OddInput <- F; nFilters <-  nEmbedDim/2 }
  if(nEmbedDim %% 2 == 1){ OddInput <- T; nFilters <-  ceiling(nEmbedDim/2) }
  if(imageDims == 2){
    AvePoolingDownshift <- tf$keras$layers$AveragePooling2D(pool_size = as.integer(c(inputAvePoolingSize,inputAvePoolingSize)))
    myConv <- tf$keras$layers$Conv2D(filters=nFilters,
                          kernel_size = c(kernelSize,kernelSize),
                          activation = "linear",
                          strides = c(strides,strides),
                          padding = "valid",
                          trainable = F)
    myConv$trainable <- F
    GlobalMaxPoolLayer <- tf$keras$layers$GlobalMaxPool2D(data_format="channels_last",name="GlobalMax")
    GlobalAvePoolLayer <- tf$keras$layers$GlobalAveragePooling2D(data_format="channels_last",name="GlobalAve")
  }
  if(imageDims == 3){
    AvePoolingDownshift <- tf$keras$layers$AveragePooling3D(pool_size = as.integer(c(1L, inputAvePoolingSize,inputAvePoolingSize)))
    # 3D conv method
    if(T == T){
      myConv <- tf$keras$layers$Conv3D(filters = nFilters,
                                       kernel_size = c(temporalKernelSize, kernelSize, kernelSize),
                                       activation = "linear",
                                       strides = c(1L,strides, strides), padding = "valid", trainable = F)
      GlobalMaxPoolLayer <- tf$keras$layers$GlobalMaxPool3D(data_format="channels_last", name="GlobalMax")
      GlobalAvePoolLayer <- tf$keras$layers$GlobalAveragePooling3D(data_format="channels_last", name="GlobalAve")
    }

    # 2D conv + LSTM method
    if(T == F){
      myConv <- tf$keras$layers$ConvLSTM2D(filters = as.integer(nFilters),
                                       kernel_size = as.integer(c(kernelSize,kernelSize)),
                                       strides = as.integer(c(strides,strides)), padding = "valid", trainable = F)
      #myConv <- function(m){tf$expand_dims(tf$expand_dims( LSTM( tf$reduce_max(CONV(m),2L:3L) ),1L),1L)}
      GlobalMaxPoolLayer <- tf$keras$layers$GlobalMaxPool2D(data_format="channels_last",name="GlobalMax")
      GlobalAvePoolLayer <- tf$keras$layers$GlobalAveragePooling2D(data_format="channels_last",name="GlobalAve")
    }
    myConv$trainable <- F
  }
  GlobalPoolLayer <- function(z){
    z <- tf$concat(list(GlobalMaxPoolLayer(z),GlobalAvePoolLayer(z)),1L)
    if(OddInput){ z <- tf$gather(z, as.integer(1L:nEmbedDim), axis = 1L) }
    return( z )
  }
  InitImageProcess <- (function(im){

    # normalize if desired
    # im <- tf$divide(tf$subtract(im, NORM_MEAN_array), NORM_SD_array)

    # downshift resolution if desired
    if(inputAvePoolingSize > 1){ im <- AvePoolingDownshift(im) }
    return( im  )
  })
  getEmbedding <- tf_function(function(im_){
    im_ <- GlobalPoolLayer ( myConv( InitImageProcess( im_ )  ) )
    return( im_  )
  } )

  embeddings <- matrix(NA,nrow = length(imageKeysOfUnits), ncol = nEmbedDim)
  last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1
      print(sprintf("[%s] %.2f%% done getting image/video embeddings", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), 100*last_i / length(imageKeysOfUnits)))

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(imageKeysOfUnits)]
      last_i <- batch_indices[length(batch_indices)]
      if(last_i == length(imageKeysOfUnits)){ ok <- T }

      batchSizeOneCorrection <- F; if(length(batch_indices) == 1){
        batch_indices <- c(batch_indices, batch_indices)
        batchSizeOneCorrection <- T
      }

      # in functional mode
      if(acquireImageMethod == "functional"){
        batch_inference <- list(
          tf$cast(acquireImageFxn(imageKeysOfUnits[batch_indices], training = F),tf$float32)
        )
      }

      if(acquireImageMethod == "tf_record"){
        setwd(orig_wd)
        batch_inference <- GetElementFromTfRecordAtIndices( indices = batch_indices,
                                                            filename = file,
                                                            nObs = length(imageKeysOfUnits),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]])
        setwd(new_wd)
        saved_iterator <- batch_inference[[2]]
        batch_inference <- batch_inference[[1]]
        # batch_inference[[2]]; batch_indices
      }

      embed_ <- try(as.matrix( getEmbedding(   batch_inference[[1]]  )  ), T)
      if("try-error" %in% class(embed_)){ browser() }
      if(batchSizeOneCorrection){ batch_indices <- batch_indices[-1]; embed_ <- embed_[1,] }
      embeddings[batch_indices,] <- embed_

    gc(); try(py_gc$collect(), T)
  }
  print(sprintf("[%s] %.2f%% done with getting image/video embeddings", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), 100*1))

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )


   return( list( "embeddings"= embeddings,
                 "embeddings_fxn" = getEmbedding ) )
}
