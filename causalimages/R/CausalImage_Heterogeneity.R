#!/usr/bin/env Rscript
#' Decompose treatment effect heterogeneity by image
#'
#' Implements the image heterogeneity decomposition analysis of Jerzak, Johansson, and Daoud (2023). Users
#' input in treatment and outcome data, along with a function specifying how to load in images
#' using keys referenced to each unit (since loading in all image data will usually not be possible due to memory limitations).
#' This function by default performs estimation, constructs salience maps, and can optionally perform
#' estimation for new areas outside the original study sites in a transportability analysis.
#'
#' @usage
#'
#' AnalyzeImageHeterogeneity(obsW, obsY, imageKeysOfUnits, acquireImageFxn, kClust_est, ...)
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param kClust_est (default = `2L`) Integer specifying the number of clusters used in estimation.
#' @param file (default = `NULL`) Path to a tfrecord file generated by `WriteTfRecord`.
#' @param acquireImageFxn A function specifying how to load images representations associated with `imageKeysOfUnits` into memory. For example, if observation `3` has a value  of `"a34f"` in `imageKeysOfUnits`, `acquireImageFxn` should extract the image associated with the unique key `"a34f"`.
#' First argument should be image key values and second argument have be `training` (in case of behavior change in training/inference).
#' @param transportabilityMat (optional) A matrix with a column named `key` specifying keys to be used by `acquireImageFxn` for generating treatment effect predictions for out-of-sample points in earth observation data settings.
#' @param imageKeysOfUnits (default = `1:length(obsY)`) A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into `acquireImageFxn` to call images into memory.
#' @param long,lat (optional) Vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.
#' @param X (optional) A numeric matrix containing tabular information used if `orthogonalize = T`.
#' @param conda_env (default = `NULL`) A string specifying a conda environment wherein `tensorflow`, `tensorflow_probability`, and `gc` are installed.
#' @param conda_env_required (default = `F`) A Boolean stating whether use of the specified conda environment is required.
#' @param orthogonalize (default = `F`) A Boolean specifying whether to perform the image decomposition after orthogonalizing with respect to tabular covariates specified in `X`.
#' @param nMonte_variational (default = `5L`) An integer specifying how many Monte Carlo iterations to use in the
#' calculation of the expected likelihood in each training step.
#' @param nMonte_predictive (default = `20L`) An integer specifying how many Monte Carlo iterations to use in the calculation
#' of posterior means (e.g., mean cluster probabilities).
#' @param nMonte_salience (default = `100L`) An integer specifying how many Monte Carlo iterations to use in the calculation
#' of the salience maps (e.g., image gradients of expected cluster probabilities).
#' @param reparameterizationType (default = `"Flipout"`) Either `"Flipout"`, or `"Reparameterization"`. Specifies the estimator used in the Bayesian neural components. With `"Flipout"`, convolutions are performed via CPU; with `"Reparameterization", they are performed by GPU if available.
#' @param figuresTag (default = `""`) A string specifying an identifier that is appended to all figure names.
#' @param figuresPath (default = `"./"`) A string specifying file path for saved figures made in the analysis.
#' @param kernelSize (default = `5L`) Dimensions used in convolution kernels.
#' @param nSGD (default = `400L`) Number of stochastic gradient descent (SGD) iterations.
#' @param batchSize (default = `25L`) Batch size used in SGD optimization.
#' @param doConvLowerDimProj (default = `T`) Should we project the `nFilters` convolutional feature dimensions down to `nDimLowerDimConv` to reduce the number of required parameters.
#' @param nDimLowerDimConv (default = `3L`) If `doConvLowerDimProj = T`, then, in each convolutional layer, we project the `nFilters` feature dimensions down to `nDimLowerDimConv` to reduce the number of parameters needed.
#' @param nFilters (default = `32L`) Integer specifying the number of convolutional filters used.
#' @param nDenseWidth (default = `32L`) Width of dense projection layers post-convolutions.
#' @param nDepthHidden_conv (default = `3L`) Hidden depth of convolutional layer.
#' @param modelClass (default = `"cnn"`) Either `"cnn"` or `"embeddings"`.
#' @param nEmbedDim (default = `96L`) Integer specifying the image/image sequence embedding dimension. Used if `modelClass = "embeddings"`.
#' @param nDepthHidden_dense (default = `0L`) Hidden depth of dense layers. Default of `0L` means a single projection layer is performed after the convolutional layer (i.e., no hidden layers are used).
#' @param quiet (default = `F`) Should we suppress information about progress?
#' @param yDensity (default = `normal`) Specifies the density for the outcome. Current options include `normal` and `lognormal`.
#' @param maxPoolSize (default = `2L`) Integer specifying the max pooling size used in the convolutional layers.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.=
#' @param simMode (default = `F`) Should the analysis be performed in comparison with ground truth from simulation?
#' @param plotResults (default = `T`) Should analysis results be plotted?
#' @param plotBands (default = `1L`) An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RGB plotting).
#' @param channelNormalize (default = `T`) Should channelwise image feature normalization be attempted? Default is `T`, as this improves training.
#' @param TfRecords_BufferScaler (default = `4L`) The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#'
#' @return Returns a list consiting of \itemize{
#'   \item `clusterTaus_mean` default
#'   \item `clusterTaus_sd` Estimated image effect cluster standard deviations.
#'   \item `clusterProbs_mean`. Estimated mean image effect cluster probabilities.
#'   \item `clusterTaus_sd`. Estimated image effect cluster probability standard deviations.
#'   \item `clusterProbs_lowerConf`. Estimated lower confidence for effect cluster probabilities.
#'   \item `impliedATE`. Implied ATE.
#'   \item `individualTau_est`. Estimated individual-level image-based treatment effects.
#'   \item `transportabilityMat`. Transportability matrix withestimated cluster information.
#'   \item `plottedCoordinates`. List containing coordinates plotted in salience maps.
#'   \item `whichNA_dropped`. A vector containing observations dropped due to missingness.
#' }
#'
#' @section References:
#' \itemize{
#' \item Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Image-based Treatment Effect Heterogeneity. Forthcoming in \emph{Proceedings of the Second Conference on Causal Learning and Reasoning (CLeaR), Proceedings of Machine Learning Research (PMLR)}, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @import tensorflow
#' @export
#' @md
AnalyzeImageHeterogeneity <- function(obsW,
                                      obsY,
                                      X = NULL,
                                      orthogonalize = F,
                                      imageKeysOfUnits = 1:length(obsY),
                                      kClust_est = 2,
                                      acquireImageFxn = NULL ,
                                      file = NULL,
                                      transportabilityMat = NULL ,
                                      lat = NULL,
                                      long = NULL,
                                      conda_env = NULL,
                                      conda_env_required = F,

                                      figuresTag = "",
                                      figuresPath = "./",
                                      plotBands = 1L,
                                      heterogeneityModelType = "variational_minimal",
                                      simMode = F,
                                      plotResults = F,

                                      nDepthHidden_conv = 1L,
                                      nDepthHidden_dense = 0L,
                                      maxPoolSize = 2L,
                                      strides = 1L,
                                      yDensity = "normal",
                                      compile = T,
                                      nMonte_variational = 5L,
                                      nMonte_predictive = 20L,
                                      nMonte_salience = 100L,
                                      batchSize = 25L,
                                      kernelSize = 5L,
                                      temporalKernelSize = 2L,
                                      nSGD  = 400L,
                                      nDenseWidth = 32L,
                                      reparameterizationType = "Reparameterization",
                                      doConvLowerDimProj = T,
                                      nDimLowerDimConv = 3L,
                                      nFilters = 32L,
                                      channelNormalize = T,
                                      modelClass = "cnn",
                                      nEmbedDim = 96L,
                                      LEARNING_RATE_BASE = 0.005,
                                      printDiagnostics = F,
                                      TfRecords_BufferScaler = 4L,
                                      dataType = "image",
                                      quiet = F){
  # note to maintainers:
  # jit only things outside of Monte Carlo steps
  # (jitting a Monte Carlo loop eats up memory)
  if(T == T){
    library(tensorflow); library(keras)
    if(!is.null(conda_env)){
      try(tensorflow::use_condaenv(conda_env, required = conda_env_required),T)
    }
    Sys.sleep(1.); try(tf$square(1.),T); Sys.sleep(1.)
    try(tf$config$experimental$set_memory_growth(tf$config$list_physical_devices('GPU')[[1]],T),T)
    tf$config$set_soft_device_placement( T )
    tfp <- tf_probability()
    tfd <- tfp$distributions
    #tfa <- reticulate::import("tensorflow_addons")

    tf$random$set_seed(  c(1000L ) )
    tf$keras$utils$set_random_seed( 4L )

    py_gc <- reticulate::import("gc")
    gc(); py_gc$collect()
  }

  # make all directory logic explicit
  orig_wd <- getwd()
  cond1 <- substr(figuresPath, start = 0, stop = 1) == "."
  cond2 <- substr(figuresPath, start = 0, stop = 1) == "/"
  if(cond1){
    figuresPath <- gsub(figuresPath, pattern = '\\.', replace = orig_wd)
  }

  acquireImageMethod <- "functional";
  # define base tf record + train/test fxns
  changed_wd <- F; if(  !is.null(  file  )  ){
    acquireImageMethod <- "tf_record"

    # established tfrecord connection
    print("Establishing connection with tfrecord")
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    new_wd <- paste(tf_record_name[-length(tf_record_name)], collapse = "/")
    print( sprintf("Temporarily re-setting the wd to %s", new_wd ) )
    changed_wd <- T; setwd( new_wd )
    tf_dataset <- tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- dataType == "video"
    getParsed_tf_dataset_inference <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L,round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
      #num_parallel_calls = tf$data$AUTOTUNE)
      dataset <- dataset$shuffle(buffer_size = tf$constant(as.integer(TfRecords_BufferScaler*batchSize),dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(  as.integer(batchSize)   )
      #dataset <- dataset$prefetch(tf$data$AUTOTUNE)
      return( dataset  )
    }

    # setup iterators
    #Sys.setenv(M_CHECK_ = "1")
    #LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
    #Sys.setenv(MALLOC_TRIM_THRESHOLD_ = "0")
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    #iterator = dataset.shuffle(int(1e7)).batch(int(1e6)).repeat(10)
    #tf_dataset_train <- tf_dataset_train$`repeat`(  as.integer(2*ceiling(batchSize*nSGD / length(obsY)  ) ) )
    tf_dataset_train <- tf_dataset_train$`repeat`(  -1L )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    if(T == F){
      # tests; see https://stackoverflow.com/questions/72552605/how-to-fix-tensorflow-datasets-memory-leak-when-shuffling
      ds_next_train <- reticulate::iter_next( ds_iterator_train )
      batch_indices <- as.array(ds_next_train[[2]])
    }
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )

    # checks
    # ds_iterator_inference$output_shapes; ds_iterator_train$output_shapes
    # ds_next_train <- reticulate::iter_next( ds_iterator_train )
    # ds_next_inference <- reticulate::iter_next( ds_iterator_inference )
  }

  # coerce output to tf$constant
  if(acquireImageMethod == "functional"){
    environment(acquireImageFxn) <- environment()
    test_ <- acquireImageFxn(imageKeysOfUnits[1:5],training = F)
    if(!"tensorflow.tensor" %in% class(test_)){
      acquireImageFxn_as_input <- acquireImageFxn
      acquireImageFxn <- function(keys, training){
        m_ <- tf$constant(acquireImageFxn_as_input(keys, training),tf$float32)
        if(length(m_$shape) == 3){
          # expand across batch dimension if receiving no batch dimension
          m_ <- tf$expand_dims(m_,0L)
        }
        return( m_ )
      }
    }
    rm( test_ )
  }
  if(channelNormalize == T){
    print("Getting channel normalization parameters...")
    tmp <- replicate(30, {
        if(acquireImageMethod == "functional"){
          tmp <- acquireImageFxn( sample(unique(imageKeysOfUnits), batchSize), training = F)
        }
        if(acquireImageMethod == "tf_record"){
           tmp <- reticulate::iter_next( ds_iterator_train )[[1]]
        }
        if(length(dim(tmp)) == 4){
          CausalImagesDataType <<- "image"
          l_ <- list("NORM_MEAN" = apply(as.array(tmp),4,function(zer){mean(zer,na.rm=T)}),
               "NORM_SD" = apply(as.array(tmp),4,function(zer){sd(zer,na.rm=T)})  )
        }
        if(length(dim(tmp)) == 5){
          CausalImagesDataType <<- "video"
          nTimeSteps <<- dim(tmp)[2]
          l_ <- list("NORM_MEAN" = apply(as.array(tmp),5,function(zer){mean(zer,na.rm=T)}),
                     "NORM_SD" = apply(as.array(tmp),5,function(zer){sd(zer,na.rm=T)})  )
        }
        return( l_  )
    })
    NORM_MEAN <- tf$expand_dims( tf$expand_dims(tf$expand_dims(tf$constant(colMeans(do.call(rbind,tmp["NORM_MEAN",]))),0L),0L), 0L)
    NORM_SD <- tf$expand_dims( tf$expand_dims(tf$expand_dims(colMeans(do.call(rbind,tmp["NORM_SD",])),0L),0L), 0L)
    if(length(dim(tmp)) == 5){
      NORM_MEAN <- tf$expand_dims( NORM_MEAN, 0L)
      NORM_SD <- tf$expand_dims( NORM_SD, 0L)
    }
    InitImageProcess <- tf_function(  function(m, training){ (m - NORM_MEAN) / NORM_SD } )
    rm(  tmp  )
  }

  print2 <- function(x){if(!quiet){print(x)}}

  # set environment of image sampling functions
  figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")
  nDimLowerDimConv <- as.integer( nDimLowerDimConv )
  windowCounter <- 0

  # orthogonalize if specified
  whichNA_dropped <- c()
  if(orthogonalize){
    print2("Orthogonalizing Potential Outcomes...")
    if(is.null(X)){stop("orthogonalize set to TRUE, but no X specified to perform orthogonalization!")}

    # drop observations with NAs in their orthogonalized outcomes
    whichNA_dropped <- which( is.na(  rowSums( X ) ) )
    if(length(whichNA_dropped) > 0){
      # note: transportabilityMat doesn't need to drop dropNAs
      obsW <- obsW[-whichNA_dropped]
      obsY <- obsY[-whichNA_dropped]
      X <- X[-whichNA_dropped,]
      imageKeysOfUnits <- imageKeysOfUnits[-whichNA_dropped]
      lat <- lat[ -whichNA_dropped ]
      long <- long[ -whichNA_dropped ]
    }
    Yobs_ortho <- resid(temp_lm <- lm(obsY ~ X))
    if(length(Yobs_ortho) != length(obsY)){
      stop("length(Yobs_ortho) != length(obsY)")
    }
    plot(obsY,Yobs_ortho)
    obsY <- Yobs_ortho
    #YandW_mat[f2n(names(my_resid)),]$Yobs_ortho <- as.numeric(YandW_mat[f2n(names(my_resid)),]$Yobs_ortho)
    #YandW_mat[["Yobs_ortho"]][f2n(names(my_resid))] <- my_resid
    #try({plot(YandW_mat$Yobs_ortho,YandW_mat$Yobs);abline(a=0,b=1)},T)
    #YandW_mat$Yobs_ortho[is.na(YandW_mat$Yobs_ortho)] <- mean(YandW_mat$Yobs_ortho,na.rm=T)
  }

  # setup tf record
  loss_vec <- NULL

  # specify some training parameters + helper functions
  rzip <- function(l1,l2){  fl<-list(); for(aia in 1:length(l1)){ fl[[aia]] <- list(l1[[aia]], l2[[aia]]) }; return( fl  ) }
  GlobalMax <- tf$keras$layers$GlobalMaxPool2D()
  GlobalAve <- tf$keras$layers$GlobalAveragePooling2D()
  GlobalFlatten <- tf$keras$layers$Flatten()
  FinalImageSummary <- function(x){tf$concat(list(GlobalMax(x),GlobalAve(x)),1L)}
  #GlobalSpatial <- tfa$layers$SpatialPyramidPooling2D(bins = list(4L,4L))
  #FinalImageSummary <- function(x){GlobalFlatten( GlobalSpatial( x ) )}
  #FinalImageSummary <- GlobalFlatten

  adaptiveMomentum <- F
  BNPreOutput <- F;
  BNPrePreOutput <- T;
  LowerDimActivation <- ConvActivation <- "swish"
  LowerDimInputDense <- F
  doBN_conv1 <- T; doBN_conv2 <- T
  kernelSize_est <- as.integer(  kernelSize )
  batchFracOut <- max(1/3*batchSize,3) / batchSize
  nMonte_variational <- as.integer( nMonte_variational  )
  widthCycle <- 50
  WhenPool <- c(1,2)
  TEMP_GLOBAL <- 5.
  #as the temperature goes to 0 the RelaxedOneHotCategorical becomes discrete with a distribution described by the logits or probs parameters
  #plot(as.matrix(do.call(rbind,replicate(10,tfd$RelaxedOneHotCategorical(temperature = TEMP_GLOBAL, probs = c(0.1,0.9))$sample(1L))))[,2],ylim = c(0,1))
  #points(as.matrix(do.call(rbind,replicate(10,tfd$RelaxedOneHotCategorical(temperature = TEMP_GLOBAL, probs = c(0.5,0.5))$sample(1L))))[,2],pch = 2,col="gray")
  #points(as.matrix(do.call(rbind,replicate(10,tfd$RelaxedOneHotCategorical(temperature = TEMP_GLOBAL, probs = c(0.1,0.9))$sample(1L))))[,2],pch = 1,col="black")
  BN_MOM <- 0.9
  BN_EP <- 0.01
  if(grepl(heterogeneityModelType,pattern = "variational")){ BN_MOM <- 0.90^(1/nMonte_variational) }
  ConvNormText <- function(dim_){"tf$keras$layers$BatchNormalization(axis = 3L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP)"}
  #ConvNormText <- function(dim_){dim_ <- paste(as.character(dim_),"L",sep=""); sprintf("tfa$layers$GroupNormalization(groups = c(%s), center = T, scale = T, epsilon = BN_EP)", dim_) }
  #ConvNormText <- function(dim_){"tf$keras$layers$LayerNormalization(center = T, scale = T, epsilon = BN_EP)"}
  #tmp <- eval(parse(text = ConvNormText))
  # c(mean(c(as.array((tmp_[,,,1])))), sd(c(as.array((tmp_[,,,1])))))
  #DenseNormText <- function(){"tf$keras$layers$LayerNormalization(center = T, scale = T, epsilon = BN_EP)"}
  DenseNormText <- function(){"tf$keras$layers$BatchNormalization(center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP)"}

  # set up some placeholders
  y0_true <- r2_y1_out <- r2_y0_out <- ClusterProbs_est <- NULL
  tau_i_est <- sd_tau1 <- sd_tau2 <- negELL <- y1_est <- y0_est <- y1_true <- y0_true <- NULL
  if(!"ClusterProbs" %in% ls() &
     !"ClusterProbs" %in% ls(envir=globalenv())){ClusterProbs<-NULL}

  # normalize outcomes for stability (estimates are re-normalized after training)
  if( yDensity == "normal"){
    Y_mean <- mean(obsY); Y_sd <- sd(obsY)
    obsY <- (obsY - Y_mean)  /  Y_sd
  }
  if( yDensity == "lognormal"){
    Y_mean <- -abs(min(obsY))-0.1; Y_sd <- sd(obsY)
    obsY <- (obsY - Y_mean)  /  Y_sd
  }
  Rescale <- function(x,doMean = F){ return( x*Y_sd + ifelse(doMean, yes = Y_mean, no = 0) ) }
  Tau_mean_init_prior <- Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
  Tau_sd_init <- sqrt( var(obsY[obsW==1]) + var( obsY[obsW==0]) )
  Y0_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); return(sd(obsY[top_][obsW[top_]==0])) }))
  Y1_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); sd(obsY[top_][obsW[top_]==1]) }))
  tau_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); mean(obsY[top_][obsW[top_]==1]) - mean(obsY[top_][obsW[top_]==0]) }))
  Y0_mean_init_prior <- Y0_mean_init <- mean(obsY[obsW==0]); Y0_sd_init_prior <- Y0_sd_init <- max(0.01, median(Y0_sd_vec,na.rm=T))
  Y1_mean_init_prior <- Y1_mean_init <- mean(obsY[obsW==1]); Y1_sd_init_prior <- Y1_sd_init <- max(0.01, median(Y1_sd_vec,na.rm=T))
  Y0_sd_init_prior <- tfp$math$softplus_inverse(Y0_sd_init_prior)
  Y1_sd_init_prior <- tfp$math$softplus_inverse(Y1_sd_init_prior)

  for(BAYES_STEP in c(1,2)){
    if(BAYES_STEP == 1){ print2("Empirical Bayes Calibration Step (see  Krishnan et al. (2020))...") }
    if(BAYES_STEP == 2){ print2("Empirical Bayes Estimation Step...") }

    if(BAYES_STEP == 1){
      nSGD_ORIG <- nSGD
      nSGD <- nSGD
      L2_grad_scale <- 0.5
      SD_PRIOR_MODEL <- .01; KL_wt <- 0
      PRIOR_MODEL_FXN <- function(name_){
        eval(parse(text = 'function(dtype, shape, name, trainable, add_variable_fn){
      d_prior <- tfd$Normal(loc = tf$zeros(shape), scale = SD_PRIOR_MODEL)
      tfd$Independent(d_prior, reinterpreted_batch_ndims = tf$size(d_prior$batch_shape_tensor())) }'))
      }
      PRIOR_MODEL_FXN("hap")
    }
    if(BAYES_STEP == 2){
      nSGD <- nSGD_ORIG
      L2_grad_scale <- 0.5
      KL_wt <- batchSize / length(obsY)
      PRIOR_MODEL_FXN <- function(name_){
        prior_loc_name <- sprintf("%s_PRIOR_MEAN_HASH818",name_ )
        prior_SD_name <- sprintf("%s_PRIOR_SD_HASH818",name_ )
        ZERO_LEN_IN <- length( eval(parse(text = sprintf("%s$variables",name_)))) == 0
        if( ZERO_LEN_IN){
          prior_loc_name <- "tf$zeros(shape)"
          prior_SD_name <- "1"
        }
        #if( ZERO_LEN_IN & BAYES_STEP == 2){ browser()  }
        if( !ZERO_LEN_IN){
          z_name_ref <- eval(parse(text = sprintf("%s$variables[[1]]$name",name_)))
          z_name_ref <- gsub(z_name_ref, pattern = ":",replace = "XCOLX")
          z_name_ref <- gsub(z_name_ref, pattern = "/",replace = "XDASHX")

          # set mean
          eval.parent(parse(text = sprintf("%s <- tf$constant(%s$variables[[1]],tf$float32)",prior_loc_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(RollMean_%s)",prior_loc_name,z_name_ref)))

          # assign variable to its mean - can speed up training, but generates instabilities in optimization
          #eval.parent(parse(text = sprintf("%s$variables[[1]]$assign( %s )",name_, prior_loc_name)))

          # set sd
          # different variance scaling elections
          #eval.parent(parse(text = sprintf("%s <- tf$constant(2*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32)",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32)",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(0.1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32)",prior_SD_name,name_)))# previous use
          eval.parent(parse(text = sprintf("%s <- tf$maximum(0.001,tf$constant(0.1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32))",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- 0.1*tf$ones( tf$shape(%s$variables[[1]]),tf$float32)",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(tf$sqrt(tf$maximum(1e-4,RollVar_%s)))",prior_SD_name,z_name_ref)))
        }
        eval(parse(text = sprintf('function(dtype, shape, name, trainable, add_variable_fn){
              d_prior <- tfd$Normal(loc = (%s),
                                  scale = (%s))
              tfd$Independent(d_prior, reinterpreted_batch_ndims = tf$size(d_prior$batch_shape_tensor())) }',
                                  prior_loc_name, prior_SD_name)))
      }

      # set other priors
      Tau_mean_init_prior <- as.vector(MeanDist_tau[k_,"Mean"][[1]])
      Y0_sd_init_prior <- as.vector(SDDist_Y0[k_,"Mean"][[1]])
      Y1_sd_init_prior <- as.vector(SDDist_Y1[k_,"Mean"][[1]])
    }
    print2("Building clustering model...")
    {
      BNLayer_Axis1_Clust <-  eval(parse(text = DenseNormText()))
      BNLayer_Axis1_Y0 <- eval(parse(text = DenseNormText()))
      BNLayer_Axis1_Proj <- eval(parse(text = DenseNormText()))
      BNLayer_Axis1_ProjY0 <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                                 beta_initializer = tf$constant_initializer(Y0_mean_init),
                                                                 gamma_initializer = tf$constant_initializer(Y0_sd_init),
                                                                 name = "BN_Y0")
      BNLayer_Axis1_ProjTau <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                                  beta_initializer = tf$constant_initializer(Tau_mean_init),
                                                                  gamma_initializer = tf$constant_initializer(Tau_sd_init))
      tmp_ <- 1*1/length(obsW[obsW==1])*var(obsY[obsW==1])+1/length(obsW[obsW==0])*var(obsY[obsW==0])
      if(heterogeneityModelType == "variational_CNN"){ for(k___ in 1:kClust_est){
        eval(parse(text = sprintf("BNLayer_Axis1_Tau%s <- %s", k___, DenseNormText())))
        eval(parse(text =
                     sprintf("BNLayer_Axis1_ProjTau%s <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                               beta_initializer = tf$constant_initializer(Tau_mean_init),
                                                               gamma_initializer = tf$constant_initializer(Tau_sd_init))",k___)))
        eval(parse(text=
                     sprintf("TauProj%s = ProbDenseType(as.integer(1L),
                          kernel_prior_fn = PRIOR_MODEL_FXN('TauProj%s'),
                          name = 'TauProj%s',
                          activation='linear')",k___, k___,k___)))
      }}
      if(reparameterizationType == "Flipout"){
        #ProbLayerExecutionDevice <- '/GPU:0' # currently unavailable with tensorflow 2.11
        ProbLayerExecutionDevice <- '/CPU:0'
        ProbConvType <- tfp$layers$Convolution2DFlipout # more efficient, must wrap execution in with(tf$device('/CPU:0'),{...})
        ProbDenseType <-  tfp$layers$DenseFlipout # more efficient, must wrap execution in with(tf$device('/CPU:0'),{...})
      }
      if(reparameterizationType == "Reparameterization"){
        ProbLayerExecutionDevice <- '/GPU:0'
        ProbConvType <- tfp$layers$Convolution2DReparameterization # less efficient
        ProbDenseType <-  tfp$layers$DenseReparameterization # less efficient
      }
      for(conv_ in 1:nDepthHidden_conv){
        eval(parse(text = sprintf("BNLayer_Axis3_Clust_%s <- %s",conv_,ConvNormText(nFilters) )))
        eval(parse(text = sprintf("BNLayer_Axis3_Y0_%s <- %s",conv_,ConvNormText(nFilters) )))
        tmp <- conv_ == nDepthHidden_conv & (LowerDimInputDense == F)
        ProjNormInput <- ifelse(tmp, yes = nFilters, no = nDimLowerDimConv)
        eval(parse(text = sprintf("BNLayer_Axis3_Clust_Proj_%s <- %s",conv_,ConvNormText(ProjNormInput) )))
        eval(parse(text = sprintf("BNLayer_Axis3_Y0_Proj_%s <- %s",conv_,ConvNormText(ProjNormInput)  )))
        eval(parse(text = sprintf("ClusterConv%s <- ProbConvType(filters = nFilters,
                                                   kernel_size = kernelSize_est,
                                                   activation = ConvActivation,
                                                   kernel_prior_fn = PRIOR_MODEL_FXN('ClusterConv%s'),
                                                   strides = strides,
                                                   name = 'ClusterConv%s',
                                                   padding = 'valid')",conv_,conv_,conv_)))
        eval(parse(text = sprintf("ClusterConvProj%s <- ProbDenseType(nDimLowerDimConv,
                                kernel_prior_fn = PRIOR_MODEL_FXN('ClusterConvProj%s'),
                                activation=LowerDimActivation,
                                name = 'ClusterConvProj%s')",conv_,conv_,conv_)))
        if(grepl(heterogeneityModelType,pattern="variational")){
          eval(parse(text = sprintf("Y0Conv%s <- ProbConvType(filters = nFilters,
                                                     kernel_size=c(kernelSize_est,kernelSize_est),
                                                     activation = ConvActivation,
                                                     kernel_prior_fn = PRIOR_MODEL_FXN('Y0Conv%s'),
                                                     strides = strides,
                                                     name = 'Y0Conv%s',
                                                     padding = 'valid')",conv_,conv_,conv_)))
          eval(parse(text = sprintf("Y0ConvProj%s <- ProbDenseType(nDimLowerDimConv,
                                  kernel_prior_fn = PRIOR_MODEL_FXN('Y0ConvProj%s'), activation=LowerDimActivation)",conv_,conv_)))
          if(heterogeneityModelType == "variational_CNN"){ for(k____ in 1:kClust_est){
            eval(parse(text = sprintf("TauConv%s_%s <- ProbConvType(filters = nFilters,
                                                     kernel_size=c(kernelSize_est,kernelSize_est),
                                                     activation=ConvActivation,
                                                     kernel_prior_fn = PRIOR_MODEL_FXN('TauConv%s_%s'),
                                                     strides = strides,
                                                     name = 'TauConv%s_%s',
                                                     padding = 'valid')",conv_,k____, conv_, k____,conv_, k____)))
            eval(parse(text = sprintf("TauConvProj%s_%s <- ProbDenseType(nDimLowerDimConv,
                                    kernel_prior_fn = PRIOR_MODEL_FXN('TauConvProj%s_%s'),
                                    activation=LowerDimActivation)",conv_,k____,conv_,k____)))
            eval(parse(text = sprintf("BNLayer_Axis3_Tau_%s_%s <- %s",conv_,k____,ConvNormText(nFilters))))
            eval(parse(text = sprintf("BNLayer_Axis3_Tau_Proj_%s_%s <- %s",conv_,k____,ConvNormText(nDimLowerDimConv))))
          }}
        }
        if(heterogeneityModelType == "tarnet"){
          eval(parse(text = sprintf("Y0Conv%s <- tf$keras$layers$Conv2D(filters = nFilters,
                                                       kernel_size=c(kernelSize_est,kernelSize_est),
                                                       activation=ConvActivation,
                                                       kernel_prior_fn = PRIOR_MODEL_FXN('Y0Conv%s'),
                                                       strides = strides,
                                                       name = 'Y0Conv%s',
                                                       padding = 'valid')",conv_,conv_,conv_)))
          eval(parse(text = sprintf("Y0ConvProj%s <- ProbDenseType(nDimLowerDimConv,
                                  kernel_prior_fn = PRIOR_MODEL_FXN('Y0ConvProj%s'),
                                  name = 'Y0ConvProj%s',
                                  activation=LowerDimActivation)",conv_,conv_,conv_)))
        }
      }
      if(nDepthHidden_dense > 0){
      for(dense_ in 1:nDepthHidden_dense){
        eval(parse(text = sprintf("BNLayer_Axis1_Clust_%s <- %s",dense_,DenseNormText())))
        eval(parse(text = sprintf("BNLayer_Axis1_Y0_%s <- %s",dense_,DenseNormText())))
        eval(parse(text = sprintf("DenseProj_Clust_%s <- ProbDenseType(as.integer(nDenseWidth),
                                kernel_prior_fn = PRIOR_MODEL_FXN('DenseProj_Clust_%s'),
                                activation='swish')",dense_,dense_)))
        eval(parse(text = sprintf("DenseProj_Y0_%s <- ProbDenseType(as.integer(nDenseWidth),
                                kernel_prior_fn = PRIOR_MODEL_FXN('DenseProj_Y0_%s'),
                                activation='swish')",dense_,dense_)))

      }
      }
      ClusterProj = ProbDenseType( as.integer(kClust_est-1L),
                                   kernel_prior_fn = PRIOR_MODEL_FXN('ClusterProj'), activation='linear' )
      if(grepl(heterogeneityModelType, pattern = "variational")){Y0Proj = ProbDenseType(as.integer(1L),
                                                                      kernel_prior_fn = PRIOR_MODEL_FXN('Y0Proj'),activation='linear')}
      BNLayer_Axis1_ProjY1 <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                                 beta_initializer = tf$constant_initializer( Y1_mean_init ),
                                                                 gamma_initializer = tf$constant_initializer( Y1_sd_init ) )
      if(heterogeneityModelType == "tarnet"){
        Y0Proj = tf$keras$layers$Dense(as.integer(1L), activation='linear')
        Y1Proj = tf$keras$layers$Dense(as.integer(1L), activation='linear')
      }
      SD_scaling <- 1
      Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
      Tau_means_init <- Tau_mean_init + .01*seq(-1,1,length.out=kClust_est)*max(0.01,abs(Tau_mean_init))
      Y0_sds_prior_mean <- tfp$math$softplus_inverse(SD_scaling*(Y0_sd_init))#<-SD_scaling*sd(obsY[obsW==0])))
      Y1_sds_prior_mean <- tfp$math$softplus_inverse(SD_scaling*(Y1_sd_init))#<-SD_scaling*sd(obsY[obsW==1])))

      base_mat <- as.data.frame( matrix(list(),nrow=kClust_est,ncol=3L) ); colnames( base_mat ) <- c("Mean","SD","Prior")
      SDDist_Y1 <- SDDist_Y0 <- MeanDist_tau <- base_mat
      for(k_ in 1:kClust_est){
        # prior SD - subject-matter knowledge informs this
        # set this to a small number so network starts off as nearly deterministic
        sd_init_trainableParams <- as.numeric(tfp$math$softplus_inverse(0.0001))

        MeanDist_tau[k_,"Mean"][[1]] <- list( tf$Variable(Tau_means_init[k_],trainable=T,name=sprintf("MeanTau%s_mean",k_) ) )
        MeanDist_tau[k_,"SD"][[1]] <- list( tf$Variable(sd_init_trainableParams,trainable=T,name = sprintf("MeanTau%s_sd",k_) ) )
        MeanDist_tau[k_,"Prior"][[1]] <- list( tfd$Normal(Tau_mean_init_prior, 2*sd(tau_vec) ))

        # Y0
        SDDist_Y0[k_,"Mean"][[1]] <- list( tf$Variable(1*Y0_sds_prior_mean,trainable=T,name=sprintf("SDY0%s_mean",k_) ) )
        SDDist_Y0[k_,"SD"][[1]] <- list( tf$Variable(sd_init_trainableParams,trainable=T,name=sprintf("SDY0%s_sd",k_)) )
        SDDist_Y0[k_,"Prior"][[1]] <- list( tfd$Normal(Y0_sd_init_prior,2*sd(Y0_sd_vec)))

        # Y0
        SDDist_Y1[k_,"Mean"][[1]] <- list( tf$Variable(1*Y1_sds_prior_mean,trainable=T,name=sprintf("SDY1%s_mean",k_) ) )
        SDDist_Y1[k_,"SD"][[1]] <- list( tf$Variable(sd_init_trainableParams,trainable=T,name=sprintf("SDY1%s_sd",k_)) )
        SDDist_Y1[k_,"Prior"][[1]] <- list( tfd$Normal(Y1_sd_init_prior,2*sd(Y1_sd_vec)))

        # SD posterior checks
        #hist(as.numeric(tf$nn$softplus(rnorm(1000,as.numeric((SDDist_Y0[k_,"Mean"][[1]])), sd=as.numeric(tf$nn$softplus(SDDist_Y0[k_,"SD"][[1]]))))))
        #abline(v=as.numeric(tf$nn$softplus(as.numeric((SDDist_Y0[k_,"Mean"][[1]])))),col="red")
        #abline(v=sd(obsY[obsW==0]),lwd= 2)
      }
      CategoricalPrior <- tfd$Categorical(probs=rep(1/kClust_est,kClust_est))
      LocalMax <- tf$keras$layers$MaxPool2D(pool_size = maxPoolSize)
      LocalAve <- tf$keras$layers$AveragePooling2D(pool_size = maxPoolSize)
      #LocalPool <- function(x){tf$concat(list(LocalMax(x),LocalAve(x)),3L)}
      LocalPool <- LocalMax
    }

    if(modelClass == "embeddings"){
        if(acquireImageMethod == "tf_record"){ setwd(orig_wd)  }
        acquireImageFxnEmbeds <- NULL; if(!is.null(acquireImageFxn)){
          acquireImageFxn2 <- acquireImageFxn
          assign("acquireImageFxn2", acquireImageFxn2, envir = .GlobalEnv)
          acquireImageFxnEmbeds <- function(keys,
                                            acquireImageFxn_ = acquireImageFxn2,
                                            InitImageProcess_ = InitImageProcess2,
                                            training = F){
              tf$constant(acquireImageFxn_( keys , training = F ),tf$float32)
          }
        }
          indices_ <- 1:length( imageKeysOfUnits )

          # note: MyEmbeds_ are indexed by the original data ordering, resampling happens later
          EmbeddingsFxn <- GetImageEmbeddings(
            imageKeysOfUnits = sample( unique(imageKeysOfUnits), 10),
            batchSize = min(  c(batchSize, 10) ),
            acquireImageFxn = acquireImageFxnEmbeds,
            file = file,
            dataType = dataType,
            strides = strides,
            nEmbedDim = nEmbedDim,
            kernelSize = kernelSize,
            temporalKernelSize = temporalKernelSize,
            conda_env = "tensorflow_m1",
            conda_env_required = T )$embeddings_fxn
          if(acquireImageMethod == "tf_record"){ setwd( new_wd )  }
    }

    if(compile == T){ tf_function_fxn <- function(x){tf_function(x,experimental_relax_shapes = F)}}
    if(compile == F){ tf_function_fxn <- function(x){print("Warning: Not compiling fxn");  return(x) } }
    getClusterLogits <- tf_function_fxn( function(  m , training){
      if( !heterogeneityModelType %in% "variational_minimal_visualizer" ){

        # convolution model
        if(modelClass == "cnn"){
          for(conv__ in 1L:nDepthHidden_conv){
            eval(parse(text = sprintf("m <-  with(tf$device( ProbLayerExecutionDevice ), { ClusterConv%s( m ) }) ",conv__)))
            if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
            doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
            if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Clust_%s(m,training=training)",conv__)))}
            if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { ClusterConvProj%s( m ) })",conv__))) }
            if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Clust_Proj_%s(m,training=training)",conv__)))}
          }
          print2("Final convolved image dimensions: ")
          print2(dim(m))
          m <- FinalImageSummary(m)
          m <- BNLayer_Axis1_Clust(m, training = training)
        }

        # embeddings model
        if(modelClass == "embeddings"){
           m <- EmbeddingsFxn(   m    )
        }

        # dense part
        if(nDepthHidden_dense > 0){
        for(dense_ in 1:nDepthHidden_dense){
          m_tminus1 <- m
          eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { DenseProj_Clust_%s(m)})",dense_)))
          if(!(dense_ == nDepthHidden_dense & !BNPrePreOutput)){
            eval(parse(text = sprintf("m <- BNLayer_Axis1_Clust_%s(m,training = training)",dense_)))
          }
          if(dense_ > 1 & dense_ < nDepthHidden_dense){ m <- m + m_tminus1 }
        }
        }
      }

      # final projection layer
      m <- with(tf$device( ProbLayerExecutionDevice ), { ClusterProj(m) })
      if(BNPreOutput){m <- BNLayer_Axis1_Proj(m, training = training) }
      m <- tf$concat(list( tf$zeros(list(tf$shape(m)[1],1L)), m),1L)
      return( m  )
    })

    if(heterogeneityModelType == "tarnet"){
      getImageRep <- tf_function_fxn( function(m,training){

        # convolutional model
        if(modelClass == "cnn"){
          for(conv__ in 1:nDepthHidden_conv){
            eval(parse(text = sprintf("m <-  with(tf$device( ProbLayerExecutionDevice ), { Y0Conv%s( m  ) })",conv__)))
            if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
            doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
            if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_%s(m,training=training)",conv__)))}
            if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { Y0ConvProj%s( m ) })",conv__))) }
            if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_Proj_%s(m,training=training)",conv__)))}
          }
          m <- FinalImageSummary( m  )
          m <- BNLayer_Axis1_Y0(m, training = training)
        }

        # embeddings model
        if(modelClass == "embeddings"){
          m <- EmbeddingsFxn(   m    )
        }

      } )
      getEY0 <- tf_function_fxn(function(  m , training  ){
        m <- getImageRep(m,training=training)
        return( getEY0_finalStep(m,training=training) )
      })
      getEY1 <- tf_function_fxn(function(  m , training  ){
        m <- getImageRep(m,training=training)
        return( getEY1_finalStep(m,training=training) )
      })
      getEY0_finalStep <- tf_function_fxn(function(  m , training  ){
        m <- with(tf$device( ProbLayerExecutionDevice ), { Y0Proj(m) } )
        if(BNPreOutput){m <- BNLayer_Axis1_ProjY0(m, training = training)}
        return( m  )
      } )
      getEY1_finalStep <- tf_function_fxn( function(  m , training){
        m <- with(tf$device( ProbLayerExecutionDevice ), { Y1Proj(m) } )
        if(BNPreOutput){m <- BNLayer_Axis1_ProjY1(m, training = training)}
        return( m  )
      } )
    }
    if(grepl(heterogeneityModelType, pattern = "variational")){
      getEY0 <- tf_function_fxn(function(  m , training  ){
        if(! heterogeneityModelType %in% "variational_minimal_visualizer"){

          # convolution model
          if(modelClass == "cnn"){
            for(conv__ in 1:nDepthHidden_conv){
              eval(parse(text = sprintf("m <-  with(tf$device( ProbLayerExecutionDevice ), { Y0Conv%s( m )} )",conv__)))
              if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
              doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
              if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_%s(m,training=training)",conv__)))}
              if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { Y0ConvProj%s( m ) })",conv__))) }
              if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_Proj_%s(m,training=training)",conv__)))}
            }
            m <- FinalImageSummary( m  )
            m <- BNLayer_Axis1_Y0(m, training = training)
          }

          if(modelClass == "embeddings"){
            m <- EmbeddingsFxn(   m    )
          }

          # dense part
          if(nDepthHidden_dense > 0){
          for(dense_ in 1:nDepthHidden_dense){
            m_tminus1 <- m
            eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { DenseProj_Y0_%s(m) } ) ",dense_)))
            if(!(dense_ == nDepthHidden_dense & !BNPrePreOutput)){
              eval(parse(text = sprintf("m <- BNLayer_Axis1_Y0_%s(m,training = training)",dense_)))
            }
            if(dense_ > 1 & dense_ < nDepthHidden_dense){ m <- m + m_tminus1 }
          }
          }
        }
        m <- with(tf$device( ProbLayerExecutionDevice ), { Y0Proj(m) } )
        if(BNPreOutput){m <- BNLayer_Axis1_ProjY0(m, training = training)}
        return( m  )
      } )
      getClusterProb <- tf_function_fxn(function(m , training){
        return( tf$nn$softmax(getClusterLogits(m, training = training), 1L) )
      })
      getClusterSamp_logitInput <- tf_function_fxn( function(logits_){
        clustT_samp = tfd$RelaxedOneHotCategorical(temperature = TEMP_GLOBAL, logits = logits_)$sample(1L)
      })

      if(heterogeneityModelType == "variational_CNN"){
        getTau <- tf_function_fxn(function(  m, training  ){
          m_orig <- m
          m_ret<-list();for(k____ in 1:kClust_est){

            # cnn model
            if(modelClass == "cnn"){
              for(conv__ in 1:nDepthHidden_conv){
                if(conv__ == 1){ m <- m_orig }
                eval(parse(text = sprintf("m <-  TauConv%s_%s( m )",conv__,k____)))
                if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
                doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
                if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Tau_%s_%s(m,training=training)",conv__,k____)))}
                if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { TauConvProj%s_%s( m )})",conv__,k____))) }
                if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Tau_Proj_%s_%s(m,training=training)",conv__,k____)))}
              }
              m <- FinalImageSummary( m  )
              eval(parse(text = sprintf("m <- BNLayer_Axis1_Tau%s(m, training = training)",k____)))
            }

            # cnn model
            if(modelClass == "embeddings"){
              m <- EmbeddingsFxn( m )
            }

            eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { TauProj%s(m)} )",k____)))
            if(BNPreOutput){
              eval(parse(text = sprintf("m <- BNLayer_Axis1_ProjTau%s(m, training = training)",k____)))
            }
            m_ret[[k____]] <- m
          }
          m_ret <- tf$concat(m_ret,1L)
          return( m_ret  )
        } )
        getEY1 <- tf_function_fxn( function(  m , training){
          EY0 <- getEY0(m = m,
                      training = training)
          Clust_logits <- getClusterLogits(m,training = training)
          clustT <- tf$squeeze(getClusterSamp_logitInput(Clust_logits),0L)
          Etau_i <- getTau(m, training = training)
          Etau_i <- tf$reduce_sum(tf$multiply(Etau_i, clustT), axis = 1L, keepdims=T)
          EY1 <- EY0 + Etau_i
          return(  EY1   )
        } )
      }
      if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
        getEY1 <- tf_function_fxn( function(  m , training){
          EY0 <- getEY0(m=m,training = training)
          Clust_logits <- getClusterLogits(m,training = training)
          clustT <- tf$squeeze(getClusterSamp_logitInput(Clust_logits),0L)
          ETau_draw <-  (tfd$Normal(getTau_means(),
                                    tf$nn$softplus(MeanDist_tau[,"SD"])))$sample(1L)
          Etau_i <- tf$reduce_sum(tf$multiply(ETau_draw, clustT), axis = 1L, keepdims=T)
          EY1 <- EY0 + Etau_i
          return(  EY1   )
        } )
      }
      marginal_tau <- tf$constant(mean(obsY[obsW==1],na.rm=T)-mean(obsY[obsW==0],na.rm=T), tf$float32)
      marginal_lambda <- tf$constant(.01, tf$float32)
      TauRunning <- tf$Variable(t(rep(as.matrix(marginal_tau),times=kClust_est)),
                                dtype=tf$float32,trainable=F)
      getTau_means <- tf_function_fxn( function(){
        if(yDensity == "normal"){ ret_ <- tf$identity(MeanDist_tau[,"Mean"]) }
        if(yDensity == "lognormal"){ ret_ <- tf$identity(MeanDist_tau[,"Mean"]) }
        return(ret_)
      } )
    }

    getTrainingLikelihoodDraw <- tf_function_fxn(function(dat,treat,y){
      training <- T
      nMonte_internal <- 1L

      # cluster probabilities
      Clust_logits <- replicate(nMonte_internal,
                                tf$expand_dims(getClusterLogits(dat, training = training),0L))
      Clust_logits <- tf$concat(Clust_logits,0L)
      clustT <- tf$squeeze(getClusterSamp_logitInput(Clust_logits),0L)
      Clust_probs <- tf$nn$softmax(Clust_logits, 2L)
      #CategoricalPost <- tfd$Categorical(probs = Clust_probs)

      EY0_i <- replicate(nMonte_internal, tf$expand_dims(getEY0(dat,training = training),0L))
      EY0_i <- tf$squeeze(tf$concat(EY0_i,0L),2L)

      # enforce ATE
      nBatch_dynamic <- tf$gather(dat$shape,0L)
      if(heterogeneityModelType == "variational_CNN"){
        ETau_draw <- tf$concat(replicate(nMonte_internal,
                       tf$expand_dims(getTau(dat,training = training),0L)),0L)
      }
      if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
        Tau_mean_vec <- getTau_means()
        MeanDist_Tau_post = (tfd$Normal(Tau_mean_vec,
                                        (Tau_sd_vec <- tf$nn$softplus(MeanDist_tau[,"SD"]))))
        ETau_draw <- tf$expand_dims(MeanDist_Tau_post$sample( nBatch_dynamic ),0L)
      }

      SDDist_Y1_post = (tfd$Normal(tf$identity(SDDist_Y1[,"Mean"]),
                                   tf$nn$softplus(SDDist_Y1[,"SD"])))
      EY1SD_draw <- tf$expand_dims(tf$nn$softplus(SDDist_Y1_post$sample(nBatch_dynamic)),0L)

      SDDist_Y0_post = (tfd$Normal(tf$identity(SDDist_Y0[,"Mean"]),
                                   tf$nn$softplus(SDDist_Y0[,"SD"])))
      EY0SD_draw <- tf$expand_dims(tf$nn$softplus(SDDist_Y0_post$sample(nBatch_dynamic)),0L)

      Etau_i <- try(tf$reduce_sum( tf$multiply(ETau_draw, clustT), 2L ),T)
      if('try-error' %in% class(Etau_i)){browser()}
      EY1_i <- EY0_i + Etau_i
      impliedATE <- tf$reduce_mean(Etau_i,1L)
      Sigma2_Y0_i <- tf$reduce_sum(tf$multiply( EY0SD_draw^2, clustT),2L)
      Sigma2_Y1_i <- tf$reduce_sum(tf$multiply( EY1SD_draw^2, clustT),2L)
      treat <- tf$expand_dims( treat , 0L)
      Y_Sigma <- (tf$multiply( 1 - treat , Sigma2_Y0_i ) +
                    tf$multiply( treat, Sigma2_Y1_i ))^0.5
      Y_Mean <- tf$multiply( 1 - treat, EY0_i ) +
                     tf$multiply( treat, EY1_i)

      # some commented analyses to triple-check code correctness re: initialization
      #plot(as.numeric(tf$reduce_mean(Y_Mean,0L)),as.numeric(y),col=as.numeric(treat)+1);abline(a=0,b=1)
      #lim_<-summary(c(as.numeric(tf$reduce_mean(Y_Mean,0L)),as.numeric(y)))[c(1,6)]
      #try({plot(as.numeric(tf$reduce_mean(Y_Mean,0L)),as.numeric(y),ylim=lim_,xlim=lim_,col=as.numeric(treat)+1);abline(a=0,b=1)},T)
      #print2( 1-sum( (as.numeric(tf$reduce_mean(Y_Mean,0L))-as.numeric(y))^2)/sum((as.numeric(y)-mean(as.numeric(y)))^2))
      if(yDensity == "normal"){
        likelihood_distribution_draws <- tfd$Normal(loc = Y_Mean, scale = Y_Sigma)
      }
      if(yDensity == "lognormal"){
        likelihood_distribution_draws <- tfd$LogNormal(loc = Y_Mean, scale = Y_Sigma)
      }
      likelihood_distribution_draw <- tf$reduce_mean(likelihood_distribution_draws$log_prob( tf$expand_dims(y,0L) ),0L)
      return( likelihood_distribution_draw )
    } )

    #getExpectedLikelihood <- tf_function_fxn(function(dat,treat,y){
    getExpectedLikelihood <- (function(dat,treat,y){
      likelihood_distribution_expectation <- tf$zeros(list(batchSize),tf$float32)
      for(d_ in 1:nMonte_variational){
        likelihood_distribution_expectation <- likelihood_distribution_expectation +
          getTrainingLikelihoodDraw(dat=dat,treat=treat,y=y) / tf$constant(f2n(nMonte_variational),tf$float32)
      }
      return( likelihood_distribution_expectation )
    })

    # get KL terms
    getKL <- tf_function_fxn(function(){
      # specify some distributions
      if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
        Tau_mean_vec <- getTau_means()
        MeanDist_Tau_post = (tfd$Normal(Tau_mean_vec, Tau_sd_vec <- tf$nn$softplus(MeanDist_tau[,"SD"])))
      }
      SDDist_Y1_post = (tfd$Normal(tf$identity(SDDist_Y1[,"Mean"]), tf$nn$softplus(SDDist_Y1[,"SD"])))
      SDDist_Y0_post = (tfd$Normal(tf$identity(SDDist_Y0[,"Mean"]), tf$nn$softplus(SDDist_Y0[,"SD"])))

      # generate KL components
      KLterm <- tf$zeros(list())
      if(! heterogeneityModelType  %in% c("variational_minimal_visualizer")){
        if(modelClass == "cnn"){
          for(conv_ in 1:nDepthHidden_conv){
            KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(ClusterConv%s$kernel_posterior,ClusterConv%s$kernel_prior)",conv_,conv_)))
            KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(Y0Conv%s$kernel_posterior,Y0Conv%s$kernel_prior)",conv_,conv_)))
            if((ifelse(LowerDimInputDense,yes = T, no = conv_ < nDepthHidden_conv)) & doConvLowerDimProj){
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(Y0ConvProj%s$kernel_posterior,Y0ConvProj%s$kernel_prior)",conv_,conv_)))
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(ClusterConvProj%s$kernel_posterior,ClusterConvProj%s$kernel_prior)",conv_,conv_)))
            }
            if(heterogeneityModelType == "variational_CNN"){
              for(k_ in 1:kClust_est){
                KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(TauConv%s_%s$kernel_posterior,TauConv%s_%s$kernel_prior)",conv_,k_,conv_,k_)))
                if((ifelse(LowerDimInputDense,yes = T, no = conv_ < nDepthHidden_conv)) & doConvLowerDimProj){
                  KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(TauConvProj%s_%s$kernel_posterior,TauConvProj%s_%s$kernel_prior)",conv_,k_,conv_,k_)))
                }
              }
            }
          } }

        if(nDepthHidden_dense > 0){ for(dense_ in 1:nDepthHidden_dense){
          KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Y0_%s$kernel_posterior,DenseProj_Y0_%s$kernel_prior)",nDepthHidden_dense, nDepthHidden_dense)))
          KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Clust_%s$kernel_posterior,DenseProj_Clust_%s$kernel_prior)",nDepthHidden_dense, nDepthHidden_dense)))
        }}
      }
      KLterm <- KLterm + tfd$kl_divergence(ClusterProj$kernel_posterior,ClusterProj$kernel_prior)
      KLterm <- KLterm + tfd$kl_divergence(Y0Proj$kernel_posterior,Y0Proj$kernel_prior)
      if(heterogeneityModelType == "variational_minimal"){
        KLterm <- KLterm + tf$reduce_sum(tfd$kl_divergence(MeanDist_Tau_post, (MeanDist_tau[,"Prior"][[1]])))
      }
      KLterm <- KLterm + tf$reduce_sum(tfd$kl_divergence(SDDist_Y0_post, (SDDist_Y0[,"Prior"][[1]])))
      KLterm <- KLterm + tf$reduce_sum(tfd$kl_divergence(SDDist_Y1_post, (SDDist_Y1[,"Prior"][[1]])))
      return( KLterm  )
    })

    #getLoss <- tf_function_fxn( function(dat,treat,y,training){
    getLoss <- ( function(dat,treat,y,training){
      if(heterogeneityModelType == "tarnet"){
        m <- getImageRep(dat,training = training)
        Y0_hat <- getEY0_finalStep(m,training=training)
        Y1_hat <- getEY1_finalStep(m,training=training)
        Yobs_hat <- tf$multiply(Y1_hat,tf$expand_dims(treat,1L)) +
                                  tf$multiply(Y0_hat,tf$expand_dims(1-treat,1L))
        minThis <- tf$reduce_sum(tf$square(tf$expand_dims(y,1L) - Yobs_hat))
      }

      if(grepl(heterogeneityModelType,pattern= "variational")){
        likelihood_distribution_expectation <- getExpectedLikelihood(dat = dat,
                                                                     treat = treat,
                                                                     y = y)

        #plot(as.numeric(tf$reduce_mean(likelihood_distribution_draws$log_prob( tf$expand_dims(y,0L) ),0L)),as.numeric( y  ),col = as.numeric( treat)+1)
        #optional ATE penalty: + tf$reduce_mean(tf$multiply(marginal_lambda, tf$square(marginal_tau - impliedATE)))
        minThis <- tf$negative(tf$reduce_sum( likelihood_distribution_expectation )) + KL_wt * getKL()
        minThis <- minThis / batchSize
        }
      return( minThis )
    })

    print2("Initial forward pass...")
    for(bool_ in c(T)){ # Initialize training branch only to preserve memory
      print2(bool_)
      with(tf$GradientTape() %as% tape, {
        if(acquireImageMethod == "functional"){
          batch_indices <- sample(1:length(obsW),batchSize)
          ds_next_train <- acquireImageFxn( imageKeysOfUnits[batch_indices], training = F)
        }
        if(acquireImageMethod == "tf_record"){
          ds_next_train <- reticulate::iter_next( ds_iterator_train )
          if( ds_next_train[[1]]$shape$as_list()[1] != batchSize ){
            ds_next_train <- reticulate::iter_next( ds_iterator_train )
          }
          batch_indices <- c(as.array(ds_next_train[[2]]))
          ds_next_train <- ds_next_train[[1]]
        }
        myLoss_forGrad <- getLoss( dat = InitImageProcess(ds_next_train, training = T),
                                   treat = tf$constant(obsW[batch_indices],tf$float32),
                                   y = tf$constant(obsY[batch_indices],tf$float32),
                                   training = bool_ )
      })
      trainable_variables  <- tape$watched_variables()
    }

    # don't train conv in embeddigns approach
    if(modelClass == "embeddings"){
      trainable_variables <- trainable_variables[!grepl(unlist(lapply(trainable_variables,function(zer){ zer$name })),
                                                        pattern = "conv\\d+d")]
    }
    print2(sprintf("nTrainableParams: %i",nTrainableParams <- sum(unlist(lapply(unlist(trainable_variables,recursive=F),function(zer){
      len_<-try(length((zer)),T);return( len_ ) })))))

    # define optimizer and training step
    #optimizer_tf = tf$optimizers$legacy$Adam(learning_rate = LEARNING_RATE_BASE) #$,clipnorm=1e1)
    optimizer_tf = tf$optimizers$legacy$Nadam(learning_rate = LEARNING_RATE_BASE) #$,clipnorm=1e1)
    if(adaptiveMomentum == T){
      optimizer_tf = tf$optimizers$legacy$Adam(learning_rate = 0, beta_1 = (BETA_1_INIT <- 0.1))#$,clipnorm=1e1)
      #optimizer_tf = tf$optimizers$legacy$Nadam(learning_rate=LEARNING_RATE_BASE,beta_1 = BETA_1_INIT)#$,clipnorm=1e1)
    }
    #LR_method <- "WNGrad"
    LR_method <- "constant"
    InvLR <- tf$Variable(0.,trainable  =  F)

    trainStep <-  (function(dat,y,treat, training){
      with(tf$GradientTape(watch_accessed_variables = F) %as% tape, {
        tape$watch(  trainable_variables   )
        myLoss_forGrad <<- getLoss( dat = dat,
                                    treat = treat,
                                    y = y,
                                    training = training)
      })
      my_grads <<- tape$gradient( myLoss_forGrad, trainable_variables )

      # update LR
      optimizer_tf$learning_rate$assign(   tf$constant(LEARNING_RATE_BASE*abs(cos(i/nSGD*widthCycle)  )*(i<=nSGD/2)+LEARNING_RATE_BASE*(i>nSGD/2)/(0.001+abs(i-nSGD/2)^0.2 )   ))
      L2_grad_i <<- sqrt(sum((grad_i <- as.numeric(tf$concat(lapply(my_grads,function(x) tf$reshape(x,list(-1L,1L))),0L) ))^2) )
      x_i <- as.numeric( tf$concat((lapply(trainable_variables, function(zer){tf$reshape(zer,-1L)})),0L))
      if(LR_method == "WNGrad"){ if(i == 1){
        L2_grad_init <<- L2_grad_scale*L2_grad_i
        InvLR$assign(  L2_grad_init )
        print2(sprintf("Initial LR: %.3f",1/L2_grad_init))
      }
      if(i > 1) { InvLR$assign_add( tf$divide( L2_grad_i,InvLR ) ) }
      }

      # apply gradients
      {
        optimizer_tf$apply_gradients( rzip(my_grads, trainable_variables))
        if(LR_method == "WNGrad"){ optimizer_tf$learning_rate$assign( tf$math$reciprocal( InvLR ) )}
      }

      # update momentum
      if(adaptiveMomentum == T){
        if(i >= 4){
          DENOM <- sqrt( sum((x_i-x_i_minus_1)^2))
          NUM <- sqrt( sum((grad_i-grad_i_minus_1)^2))
          LR_current <- as.numeric( optimizer_tf$learning_rate  )
          #UPPER_MOM <- 1-10^(-3)
          UPPER_MOM <- 1-10^(-2)
          MomenetumNextIter <<- max(0,min((1-sqrt(LR_current*NUM/(0.000001+DENOM)))^2,UPPER_MOM))
          #optimizer_tf$momentum$assign(MomenetumNextIter)
          optimizer_tf$beta_1$assign( MomenetumNextIter )
        }
        x_i_minus_1 <<- x_i
        grad_i_minus_1 <<- grad_i
      }
    })

    # perform training jump
    print2("Starting training...")
    if(BAYES_STEP == 2){
      eval(parse(text = sprintf("rm(%s)",paste(ls()[grepl(ls(),pattern="HASH818")] ,collapse= ',') )))
    }
    trainIndices <-sort(sample(1:length(obsW),length(obsW) - (nTest <- 0L)))
    testIndices <-sort(  (1:length(obsW))[! 1:length(obsW) %in% trainIndices] )
    if(length(testIndices)==0){testIndices <- trainIndices}
    L2grad_vec <- loss_vec <- rep(NA,times=(nSGD))
    batch_indices_list <- (sapply(1:(max(1,round((batchSize*nSGD) / length(trainIndices)))),function(zer){
      zer*length(trainIndices)+sample(1:length(trainIndices) %% ceiling(length(trainIndices)/batchSize)+1)
    }))

    # training loop
    IndicesByW <- tapply(1:length(obsW),obsW,c)
    UniqueImageKeysByW <- tapply(imageKeysOfUnits,obsW,function(zer){sort(unique(zer))})
    UniqueImageKeysByIndices <- list(tapply(which(obsW==0),imageKeysOfUnits[obsW==0],function(zer){sort(unique(zer))}),
                                     tapply(which(obsW==1),imageKeysOfUnits[obsW==1],function(zer){sort(unique(zer))}))
      #tauMeans <- c();i_<-1;for(i in i_:(n_sgd_iters <- length(unique_batch_indices <- sort(unique(c(batch_indices_list)))))){
      n_sgd_iters <- nSGD; tauMeans <- c();i_<-1;for(i in i_:nSGD){
      if(i %% 10 == 0){gc(); py_gc$collect()}
      #batch_indices <- unlist(apply(batch_indices_list == unique_batch_indices[i],2,which))
      #batch_indices_reffed <- trainIndices[batch_indices]
      #keys_SELECTED <- sample(unique(imageKeysOfUnits),batchSize)
      #batch_indices_reffed <- sapply(keys_SELECTED,function(zer){
      #f2n(sample(as.character(which(imageKeysOfUnits %in% zer)),1L)) })
      #table(  obsW[batch_indices_reffed] )

      if(acquireImageMethod == "functional"){
        batch_indices <-  c(  sapply(1:2, function(ze){
          w_keys <- UniqueImageKeysByW[[ze]]
          samp_w_keys <- sample(unique(w_keys),max(1,round(batchSize/2)))
          unlist(  lapply(UniqueImageKeysByIndices[[ze]][as.character(samp_w_keys)],function(fa){
            f2n(sample(as.character(fa),1))
          }) ) }))
        ds_next_train <- acquireImageFxn( imageKeysOfUnits[ batch_indices ], training = T)
      }
      if(acquireImageMethod == "tf_record"){
        ds_next_train <- reticulate::iter_next( ds_iterator_train )
        batch_indices <- c(as.array(ds_next_train[[2]]))
        ds_next_train <- ds_next_train[[1]]

        # if we run out of observations, reset iterator...
        RestartedIterator <- F
        if( is.null(ds_next_train) ){
          print("Re-setting iterator! (type 1)")
          ds_next_train <- reticulate::iter_next( ds_iterator_train )
          batch_indices <- c(as.array(ds_next_train[[2]]))
          ds_next_train <- ds_next_train[[1]]
        }

        if(!RestartedIterator){
          if(length(batch_indices) != batchSize){
            # get a new batch if size mismatch - size mismatches generate new cached compiled fxns
            print("Re-setting iterator! (type 2)")
            ds_next_train <- reticulate::iter_next( ds_iterator_train )
            batch_indices <- c(as.array(ds_next_train[[2]]))
            ds_next_train <- ds_next_train[[1]]
          }
        }
      }

      #table(YandW_mat$geo_long_lat_key[batch_indices_reffed])
      # checks via e1 and e2 for embeddings case
      # e1 <- EmbeddingsFxn(InitImageProcess(acquireImageFxn( imageKeysOfUnits[batch_indices_reffed], training = F ),F)
      trainStep(dat = InitImageProcess(ds_next_train, training = T),
                y = tf$constant(obsY[batch_indices], tf$float32),
                treat = tf$constant(obsW[batch_indices], tf$float32),
                training = T)
      # e2 <- EmbeddingsFxn(InitImageProcess(acquireImageFxn( imageKeysOfUnits[batch_indices_reffed], training = F ),F)
      # e1 - e2 # (should be 0)
      loss_vec[i] <- myLoss_forGrad <- as.numeric( myLoss_forGrad )
      L2grad_vec[i] <- as.numeric( L2_grad_i )
      if(is.na(myLoss_forGrad)){
        browser()
        stop("Stopping: NA in loss function! Perhaps batchSize is too small?")
      }
      i_ <- i ; if(i %% 20 == 0 | i < 10 ){
        print2(sprintf("SGD iteration %i of %i",i,n_sgd_iters));par(mfrow = c(1,1));
        if(!quiet){
          try({plot(loss_vec,log="y",main="If Still Decreasing at End of Training, \n Try Increasing nSGD",cex.main = 0.95,ylab = "Loss Function Value",xlab="SGD Iteration Number");points(smooth.spline( na.omit(loss_vec) ),col="red",type = "l",lwd=5)},T)
        }
        if(heterogeneityModelType == "variational_minimal"){ print2( paste("Current estimate, tau cluster means: ", paste(round(as.numeric(getTau_means()),3L),collapse=", "), collapse =  "")) }
      }
      if(BAYES_STEP == 1){
      if(abs(i - n_sgd_iters - 1) <= (nWindow <- 20)){
        windowCounter <- windowCounter + 1
        z_counter <- 0
        for(z in trainable_variables){
          z_counter <- z_counter + 1
          z_name_orig <- z_name_ <- z$name
          z_name_ <- gsub(z_name_, pattern = ":",replace = "XCOLX")
          z_name_ <- gsub(z_name_, pattern = "/",replace = "XDASHX")
          #https://monolix.lixoft.com/tasks/standard-error-using-the-fisher-information-matrix/
          if(windowCounter == 1){
            eval(parse(text = sprintf("SZ_%s <- tf$zeros(tf$shape(z))", z_name_)))
            eval(parse(text = sprintf("SZ2_%s <- tf$zeros(tf$shape(z))", z_name_)))
          }
          #eval(parse(text = sprintf("SZ_%s <- z + SZ_%s", z_name_, z_name_)))
          #eval(parse(text = sprintf("SZ2_%s <- tf$square(z) + SZ2_%s", z_name_, z_name_)))
          eval(parse(text = sprintf("SZ_%s <- my_grads[[z_counter]] + SZ_%s", z_name_, z_name_)))
          eval(parse(text = sprintf("SZ2_%s <- tf$square( my_grads[[z_counter]] ) + SZ2_%s", z_name_, z_name_)))
          if(i == n_sgd_iters){
            #eval(parse(text = sprintf("RollMean_%s <- (SZ_%s)/nWindow", z_name_, z_name_)))
            #eval(parse(text = sprintf("RollVar_%s <- tf$maximum(0.0001,SZ2_%s/nWindow - RollMean_%s^2)", z_name_,z_name_,z_name_,z_name_)))
            eval(parse(text = sprintf("RollVar_%s <- 1/tf$maximum(0.5,length(obsY)*SZ2_%s/nWindow)", z_name_,z_name_)))
          }
      }
      }
      }
    }
  }
  try(plot(L2grad_vec),T)
  #try({par(mfrow = c(1,1));plot(loss_vec, xlab = "SGD Iteration", ylab = "Loss Function Value");try(points(smooth.spline( na.omit(loss_vec) ),col="red",type = "l",lwd=5),T)},T)
  print2("Getting predicted potential outcome means...")
  for(y_t_ in c(0,1)){
    test_tab <- sort( 1:length(testIndices)%%round(length(testIndices)/max(1,round(batchFracOut*batchSize))));
    passedIterator <- NULL
    print(  sprintf("Getting Y(%s)'s",y_t_)   )
    Y_test_est <-  tapply(testIndices,test_tab,function(zer){
      gc(); py_gc$collect()
      atP <- max(zer)/length(test_tab)
      if( any(zer %% 100 == 0) ){ print2(sprintf("[%s] Proportion done: %.3f",
                                                 format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                                                 atP)) }
        if(acquireImageMethod == "functional"){
          ds_next_in <- acquireImageFxn(  transportabilityMat$key[zer], training = F )
        }
        if(acquireImageMethod == "tf_record"){
          #setwd(orig_wd)
          ds_next_in <- GetElementFromTfRecordAtIndices( indices = zer,
                                                          filename = file,
                                                          iterator = passedIterator,
                                                          readVideo = useVideo,
                                                          nObs = length(imageKeysOfUnits),
                                                          return_iterator = T)
          passedIterator <<- ds_next_in[[2]]
          ds_next_in <- ds_next_in[[1]]
          #setwd(new_wd)
          if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          ds_next_in <- ds_next_in[[1]]
        }
      im_zer <- InitImageProcess(ds_next_in, training = F)
      l_ <- replicate(nMonte_predictive,
                      eval(parse(text = sprintf("list(tf$expand_dims(getEY%s(m=im_zer, training = F),0L))",y_t_))))
      names(l_) <- NULL;
      l_ <- tf$concat(l_,0L)
      as.matrix(tf$reduce_mean(l_,0L))
    })
    Y_test_est <- do.call(rbind,Y_test_est)
    eval(parse(text = sprintf("Y%s_test_est <- Y_test_est",y_t_)))

    # get out of sample predictions
    Y_test_truth <- obsY[testIndices]
    W_test <- obsW[testIndices]
    yt_true <- Y_test_truth[W_test==y_t_]
    yt_est <- as.numeric(Y_test_est)[W_test==y_t_]
    yt_lims <- summary(c(yt_true,yt_est))[c(1,6)]
    if(printDiagnostics == T){
      #print((summary(lm(yt_true~yt_est))))
    }
    r2_yt_out <- 1 - sum( (yt_est - yt_true)^2 ) / sum( (yt_true - mean(yt_true))^2 )
    if(y_t_ == 0){ r2_y0_out <- r2_yt_out }
    if(y_t_ == 1){ r2_y1_out <- r2_yt_out }
    rm( Y_test_est )
  }

  try({plot(y0_true,y0_est,ylim = y0_lims,xlim=y0_lims);abline(a=0,b=1,lty=2,col="gray")},T)
  try({plot(y1_true,y1_est,ylim = y1_lims,xlim=y1_lims);abline(a=0,b=1,lty=2,col="gray")},T)

  rm(optimizer_tf);gc(); py_gc$collect()

  # get cluster probs
  print2("Getting predicted cluster probabilities....")
  batch_indices_tab <- sort( 1:length(obsY)%%round(length(obsY)/max(1,ceiling(batchFracOut*batchSize))))
  if(heterogeneityModelType == "tarnet" | heterogeneityModelType=="variational_CNN"){
    passedIterator <- NULL
    Y0_est <- do.call(rbind,tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
      gc(); py_gc$collect()
      atP <- max(indi_/length(obsY))
      if( any(zer %% 100 == 0) ){ print2(sprintf("[%s] Proportion Done: %.3f",
                                                 format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                                                 atP)) }
        if(acquireImageMethod == "functional"){
          ds_next_in <- acquireImageFxn(  transportabilityMat$key[indi_], training = F )
        }
        if(acquireImageMethod == "tf_record"){
          #setwd(orig_wd)
          ds_next_in <- GetElementFromTfRecordAtIndices( indices = indi_,
                                                         filename = file,
                                                         iterator = passedIterator,
                                                         readVideo = useVideo,
                                                         nObs = length(imageKeysOfUnits),
                                                         return_iterator = T)
          passedIterator <<- ds_next_in[[2]]
          ds_next_in <- ds_next_in[[1]]
          #setwd(new_wd)
          if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          ds_next_in <- ds_next_in[[1]]
        }
      im_indi <- InitImageProcess(ds_next_in, training = F)
      as.matrix(tf$reduce_mean(tf$concat(replicate(nMonte_predictive,getEY0(im_indi,training = F)),1L),1L))
    }))

    passedIterator <- NULL
    Y1_est <- do.call(rbind,tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
      gc(); py_gc$collect()
      atP <- max(indi_/length(obsY))
      if( any(zer %% 100 == 0) ){ print2(sprintf("[%s] Proportion Done: %.3f",
                                                 format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                                                 atP)) }
      if(acquireImageMethod == "functional"){
        ds_next_in <- acquireImageFxn(  transportabilityMat$key[indi_], training = F )
      }
      if(acquireImageMethod == "tf_record"){
        #setwd(orig_wd)
        ds_next_in <- GetElementFromTfRecordAtIndices( indices = indi_,
                                                       filename = file,
                                                       iterator = passedIterator,
                                                       readVideo = useVideo,
                                                       nObs = length(imageKeysOfUnits),
                                                       return_iterator = T)
        passedIterator <<- ds_next_in[[2]]
        ds_next_in <- ds_next_in[[1]]
        #setwd(new_wd)
        if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
        ds_next_in <- ds_next_in[[1]]
      }
      im_indi <- InitImageProcess(ds_next_in, training = F)
      as.matrix(tf$reduce_mean(tf$concat(replicate(nMonte_predictive,getEY1(im_indi,training = F)),1L),1L))
    }))
    Y0_est <- Rescale(Y0_est, doMean = T)
    Y1_est <- Rescale(Y1_est, doMean = T)
    tau_i_est <- Rescale( Y1_est - Y0_est, doMean = F)
    impliedATE <- mean(  tau_i_est )

    clusters_info <- kmeans(tau_i_est,centers=2)
    tau_vec <- c( Rescale(clusters_info$centers, doMean = F ) )
    tau1 <- c(tau_vec[1]); tau2 <- c(tau_vec[2])
  }

  if(grepl(heterogeneityModelType, pattern = "variational")){
    print2("Starting estimates for cluster probabilities")
    passedIterator <- NULL
    ClusterProbs_est <- tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
      atP <- max(indi_/length(obsY))
      if( any(indi_ %% 100 == 0) ){ print2(sprintf("[%s] Proportion Done: %.3f",
                                                   format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                                                   atP)) }
      if(runif(1)<0.1){ gc(); py_gc$collect() }
      if(acquireImageMethod == "functional"){
        ds_next_in <- acquireImageFxn(  transportabilityMat$key[indi_], training = F )
      }

      if(acquireImageMethod == "tf_record"){
        #setwd(orig_wd)
        ds_next_in <- GetElementFromTfRecordAtIndices( indices = indi_,
                                                       filename = file,
                                                       iterator = passedIterator,
                                                       readVideo = useVideo,
                                                       nObs = length(imageKeysOfUnits),
                                                       return_iterator = T)
        passedIterator <<- ds_next_in[[2]]
        ds_next_in <- ds_next_in[[1]]
        #setwd(new_wd)
        if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
        ds_next_in <- ds_next_in[[1]]
      }
      im_indi <- InitImageProcess(ds_next_in, training = F)
      ClusterProbs_est_ <- replicate(nMonte_predictive,as.matrix(getClusterProb(im_indi,training = F)))
      ClusterProbs_std_ <- apply(ClusterProbs_est_,1:2,function(re){sd(re,na.rm=T)})
      ClusterProbs_est_ <- apply(ClusterProbs_est_,1:2,function(re){mean(re,na.rm=T)})
      ClusterProbs_lower_conf_ <- ClusterProbs_est_ - 0. * ClusterProbs_std_
      return(list("ClusterProbs_est_"=ClusterProbs_est_,
                  "ClusterProbs_lower_conf_"=ClusterProbs_lower_conf_,
                  "ClusterProbs_std_"=ClusterProbs_std_))
    } )
    ClusterProbs_lower_conf <- do.call(rbind,lapply(ClusterProbs_est,function(zer) zer$ClusterProbs_lower_conf_))
    ClusterProbs_std <- do.call(rbind,lapply(ClusterProbs_est,function(zer) zer$ClusterProbs_std_))
    ClusterProbs_est <- lapply(ClusterProbs_est,function(zer) zer$ClusterProbs_est_)
    ClusterProbs_est <- (ClusterProbs_est_full <- do.call(rbind, ClusterProbs_est))[,2]
    Clust_probs_marginal_final <- colMeans( ClusterProbs_est_full )

    if(grepl(heterogeneityModelType,pattern="variational_minimal")){
      impliedATE <- mean(replicate(100,sum(Y_sd*as.numeric(getTau_means())*Clust_probs_marginal_final)))
    }
    gc(); py_gc$collect()

    # characterizing the treatment effects
    print2("Summarizing results...")
    SDDist_Y1_post = (tfd$Normal(tf$identity(SDDist_Y1[,"Mean"]), tf$nn$softplus(SDDist_Y1[,"SD"])))
    SDDist_Y0_post = (tfd$Normal(tf$identity(SDDist_Y0[,"Mean"]), tf$nn$softplus(SDDist_Y0[,"SD"])))
    Sigma1_sd_vec <- as.numeric(tf$reduce_mean(tf$nn$softplus(SDDist_Y1_post$sample(100L)),0L))
    Sigma0_sd_vec <- as.numeric(tf$reduce_mean(tf$nn$softplus(SDDist_Y0_post$sample(100L)),0L))

    # get uncertainties
    if(grepl(heterogeneityModelType,pattern="variational_minimal")){
      tau_vec <- as.numeric( getTau_means() ) * Y_sd
      for(k_ in 1:kClust_est){eval(parse(text = sprintf("tau%s <- tau_vec[k_]",k_))) }
      Tau_mean_vec <- getTau_means()
      MeanDist_Tau_post = (tfd$Normal(Tau_mean_vec, Tau_sd_vec <- tf$nn$softplus(MeanDist_tau[,"SD"])))
      Tau_sd_vec_ <- as.numeric(tf$sqrt(tf$math$reduce_variance(MeanDist_Tau_post$sample(100L),0L)))
      Tau_sd_vec <- sqrt(   Sigma1_sd_vec^2 + Sigma0_sd_vec^2 + Tau_sd_vec_^2 )
      Tau_sd_vec <- Tau_sd_vec * Y_sd
      sd_tau1 <- Tau_sd_vec[1]
      sd_tau2 <- Tau_sd_vec[2]
    }

    # obtaining the neg log likelihood if desired
    negELL <- NA; if(T == F){
      # obtaining the negative LL
      KL_wt_orig <- KL_wt
      if(! "function" %in% class(getLoss)){print2("getLoss must be R function for this part to work!")}
      KL_wt <- 0
      negELL <- tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
        ret_ <- as.numeric(getLoss( dat = InitImageProcess(acquireImageFxn(   imageKeysOfUnits[indi_]  , training = F),training = F),
                                    treat = tf$constant(obsW[indi_],tf$float32),
                                    y = tf$constant(obsY[indi_],tf$float32),
                                    training = F ))
        return( ret_ )
      })
      negELL <- sum(negELL)
      KL_wt <- KL_wt_orig
    }
  }

  # transportability analysis
  cluster_prob_transport_means <- NULL
  if(!is.null(transportabilityMat)){
    print2("Getting posterior predictive mean probabilities for transportability analysis...")
    {
      GetProbAndExpand <- tf_function_fxn(function(m){tf$expand_dims(getClusterProb(m,training = F),0L) })
      full_tab <- sort( 1:nrow(transportabilityMat) %% round(nrow(transportabilityMat)/max(1,round(batchFracOut*batchSize))));
      cluster_prob_transport_info <- tapply(1:nrow(transportabilityMat),full_tab,function(zer){
        gc(); py_gc$collect()
        atP <- max(  zer / nrow(transportabilityMat))
        if((round(atP,2)*100) %% 10 == 0){ print2(atP) }
        if(acquireImageMethod == "functional"){
          ds_next_in <- acquireImageFxn(  transportabilityMat$key[zer], training = F )
        }

        if(acquireImageMethod == "tf_record"){
          #setwd(orig_wd)
          ds_next_in <- GetElementFromTfRecordAtIndices( indices = zer,
                                                         filename = file,
                                                         readVideo = useVideo,
                                                         nObs = length(imageKeysOfUnits) )
          #setwd(new_wd)
          if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          ds_next_in <- ds_next_in[[1]]
        }
        im_keys <- InitImageProcess(ds_next_in, training = F)
        pred_ <- replicate(nMonte_predictive,as.array(GetProbAndExpand(im_keys) ))
        list("mean"=apply(pred_[1,,,],1:2,mean),
             "var"=apply(pred_[1,,,],1:2,var))
      })
      cluster_prob_transport_info <- do.call(rbind,cluster_prob_transport_info)
      cluster_prob_transport_means <- do.call(rbind, cluster_prob_transport_info[,1])
      cluster_prob_transport_var <- do.call(rbind, cluster_prob_transport_info[,2])
      colnames(cluster_prob_transport_means) <- paste('mean_k',1:ncol(cluster_prob_transport_means), sep = "")
      colnames(cluster_prob_transport_var) <- paste('var_k',1:ncol(cluster_prob_transport_means), sep = "")
      transportabilityMat <- try(cbind(transportabilityMat,
                                   cluster_prob_transport_means,
                                   cluster_prob_transport_var),T)
      if("try-error" %in% class(transportabilityMat)){ browser() }
    }
  }


  # perform plots
  if( changed_wd ){ setwd(  orig_wd  ) }
  if(!c("monti") %in% ls(envir = globalenv()) & simMode == F){
    pdf_name_key <- "RealDataFig"; monti <- NA
  }
  if( plotResults == T){
    if(heterogeneityModelType == "variational_minimal"){
      Tau_mean_vec_n <- as.numeric(Tau_mean_vec)
      synth_seq <- seq(min(Tau_mean_vec_n - 2 * as.numeric(Tau_sd_vec),Tau_mean_vec_n - 2 * as.numeric(Tau_sd_vec)),
                       max(Tau_mean_vec_n + 2 * as.numeric(Tau_sd_vec),Tau_mean_vec_n + 2 * as.numeric(Tau_sd_vec)),
                       length.out=1000)

      if(truthKnown <- ("ClusterProbs" %in% ls(envir = globalenv()))){
        my_density <- density(ClusterProbs)
        my_density$y <- my_density$y
        synth_seq <- seq(min(my_density$x,na.rm=T),max(my_density$x,na.rm=T),length.out=100)
      }

      for(kr_ in 1:kClust_est){
        eval(parse(text = sprintf("d%s <- dnorm(synth_seq, mean = Tau_mean_vec_n[kr_], sd = Tau_sd_vec[kr_] )",  kr_)))
      }
      pdf(sprintf("%s/HeteroSimTauDensity%s_%s_ExternalFigureKey%s.pdf",
                  figuresPath, pdf_name_key, heterogeneityModelType, figuresTag))
      {
        par(mar=c(5,5,1,1))
        numbering_seq <- 1:kClust_est #c("1","1")
        col_seq <- 1:kClust_est #c("black","black")
        #col_seq[which.max(c(as.numeric(tau1),as.numeric(tau2)))] <- "red"
        #numbering_seq[which.max(c(as.numeric(tau1),as.numeric(tau2)))] <- 2
          if(!truthKnown){
            my_density <- eval(parse(text = sprintf("
                        data.frame('y'=max(c(%s)))
                        ", paste(paste("d",1:kClust_est,sep=""),
                                 collapse = ","))  ))
          }
          plot(my_density,
               col = ifelse(truthKnown,yes="darkgray",no="white"),
               lty = 2,
               xlim = c(min(synth_seq),max(synth_seq)),
               ylim = c(0,max(my_density$y,na.rm=T)*1.5),
               cex.lab = 2, main = "",lwd = 3,
               ylab = "Density",
               xlab = "Per Image Treatment Effect")
          if(!truthKnown){ axis(2,cex.axis = 1) }
          #points(tau_vec, rep(0,times=kClust_est),
          points(Tau_mean_vec_n, rep(0,times=kClust_est),
                 col = col_seq,
                 pch = "|", cex = 4)
          for(krk_ in 1:kClust_est){
            points(synth_seq,
                   eval(parse(text = sprintf("d%s",krk_))),
                   type = "l", col= col_seq[krk_],lwd = 3)
          }
          #text(Tau_mean_vec_n, rep(max(my_density,na.rm=T)*0.1,2),
               #labels = sapply(numbering_seq, function(numbering_){
                 #eval(parse(text = sprintf("expression(hat(tau)(%s))", numbering_))) }),
               #col = col_seq, cex = 2)
          legends_seq_vec <- c(expression("True"~p(Y[i](1)-Y[i](0)~"|"~M[i])),
                               sapply(numbering_seq, function(numbering_){
                                    eval(parse(text=sprintf('
              expression(hat(p)~"("~Y[i](1)-Y[i](0)~"|"~Z[i]==%s~")")',numbering_)))}))
          lty_seq_vec <- c(2,1,1)
          col_seq_vec <- c("gray",col_seq)
          if(!truthKnown){
            legends_seq_vec <- legends_seq_vec[-1]
            col_seq_vec <- col_seq_vec[-1]
            lty_seq_vec <- lty_seq_vec[-1]
          }
          legend("topleft", legend = legends_seq_vec,
                 box.lwd = 0, box.lty = 0, cex = 2,
                 lty = lty_seq_vec, col = col_seq_vec, lwd = 3)
      }
      dev.off()

      if(truthKnown){
        order_ <- order(ClusterProbs)
        if(cor(ClusterProbs, ClusterProbs_est) > 0){
          # we do this so the coloring stays consistent
          col_dim <- rank(ClusterProbs_est)#gtools::quantcut(ClusterProbs_est, q = 100)
        }
        if(cor(ClusterProbs, ClusterProbs_est) < 0){
          col_dim <- rank(-ClusterProbs_est)#gtools::quantcut(ClusterProbs_est, q = 100)
        }
        pdf(sprintf("%s/HeteroSimClusterEx%s_ExternalFigureKey%s.pdf",
                    figuresPath, pdf_name_key, figuresTag))
        {
          par(mar=c(5,5,1,1))
          plot( ClusterProbs[order_],
                1:length(order_)/length(order_),
                xlab = "Per Image Treatment Effect",
                ylab = "Empirical CDF(x)",pch = 19,
                col = viridis::magma(n=length(col_dim),alpha=0.9)[col_dim][order_],
                cex = 1.5, cex.lab = 2)
          legend("topleft",
                 box.lwd = 0, box.lty = 0,
                 pch = 19, #box.col = "white",
                 col = c(viridis::magma(5)[2],
                         viridis::magma(5)[3],
                         viridis::magma(5)[4]),
                 cex = 2,
                 legend=c("Higher Clust 1 Prob.",
                          "...",
                          "Higher Clust 2 Prob."))
        }
        dev.off()
      }
    }
  }
  if(simMode == F){
    par(mfrow=c(1,1))
    gc(); py_gc$collect()
    try(plot(ClusterProbs_est),T)
    plotting_coordinates_list <- list(); typePlot_counter <- 0
    for(typePlot in (typePlot_vec <- c("uncertainty","mean","mean_upperConf"))){
      typePlot_counter <- typePlot_counter + 1
      rows_ <- kClust_est; nExamples <- 5
      if(typePlot == "uncertainty"){rows_ <- 1L}

      if(CausalImagesDataType == "image"){
        plot_fxn <- function(){
        pdf(sprintf("%s/VisualizeHeteroReal_%s_%s_%s_ExternalFigureKey%s.pdf",figuresPath, heterogeneityModelType,typePlot,orthogonalize,figuresTag),
            height = ifelse(grepl(typePlot,pattern = "mean"), yes = 4*rows_*3, no = 4),
            width = 4*nExamples)
        {
          #
          if(grepl(typePlot,pattern = "mean")){
            par(mar=c(2, 5.9, 3, 0.5))
            layout_mat_orig <- layout_mat <- matrix(c(1:nExamples*3-3+1,
                                   1:nExamples*3-1,
                                   1:nExamples*3), nrow = 3, byrow = T)
            for(kr_ in 2:kClust_est){
              layout_mat <- rbind(layout_mat,
                                  layout_mat_orig+max(layout_mat))
            }
          }
          if(typePlot %in% c("uncertainty")){
            par(mar=c(2,2,4,2)); layout_mat <- t(1:nExamples)
          }
          layout(mat = layout_mat,
                 widths = rep(2,ncol(layout_mat)),
                 heights = rep(2,nrow(layout_mat)))
          plotting_coordinates_mat <- c()
          total_counter <- 0
          for(k_ in 1:rows_){
            used_coordinates <- c()
            for(i in 1:5){
              #if(k_ == 2 & typePlot == "mean"){ browser() }
              #if(k_ == 2 & i == 1){ browser() }
              print2(sprintf("Type Plot: %s; k_: %s, i: %s", typePlot, k_, i))
              total_counter <- total_counter + 1
              rfxn <- function(xer){xer}
              bad_counter <- 0;isUnique_ <- F; while(isUnique_ == F){
                BreakTies <- function(x){x + runif(length(x),-1e-3,1e-3)}
                if(typePlot == "uncertainty"){
                  main_ <- letters[  total_counter  ]

                  # plot images with largest std's
                  valBrokenTies <- BreakTies(ClusterProbs_std[,k_])
                  sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                  im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                }
                if(grepl(typePlot,pattern = "mean")){
                  main_ <- total_counter

                  # plot images with largest lower confidence
                  if(typePlot ==  "mean"){
                     valBrokenTies <- BreakTies(ClusterProbs_lower_conf[,k_])
                     sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                     im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                  }

                  # plot images with largest cluster probs
                  if(typePlot ==  "mean_upperConf"){
                    valBrokenTies <- BreakTies(ClusterProbs_est_full[,k_])
                    sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                    im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                  }
                }

                coordinate_i <- c(long[im_i], lat[im_i])
                if(  bad_counter>length(obsY)  ){ browser() }
                if(  i > 1  ){
                  isUnique_ <- F; if(!is.null(long)){
                    dist_m <- geosphere::distm(coordinate_i,
                                               used_coordinates[,-1],
                                               fun = geosphere::distHaversine)
                    bad_counter <- bad_counter + 1
                    if(all(dist_m >= 1000)){isUnique_ <- T}
                } }
                if(i == 1){ isUnique_<-T }
              }

              used_coordinates <- rbind(used_coordinates, c("observation_index"=im_i, coordinate_i))
              print(sprintf("k: %i i: %i, im_i: %i, long/lat: %.3f, %.3f",
                        as.integer(k_), as.integer(i), as.integer(im_i),
                                 long[im_i], lat[im_i]))

              if(acquireImageMethod == "functional"){
                ds_next_in <- acquireImageFxn(  transportabilityMat$key[im_i], training = F )
              }
              if(acquireImageMethod == "tf_record"){
                #setwd(orig_wd)
                ds_next_in <- GetElementFromTfRecordAtIndices( indices = im_i,
                                                               filename = file,
                                                               readVideo = useVideo,
                                                               nObs = length(imageKeysOfUnits) )
                #setwd(new_wd)
                ds_next_in <- ds_next_in[[1]]
                if(length(ds_next_in$shape) == 3){ ds_next_in <- tf$expand_dims(ds_next_in, 0L) }
              }

              if(length(plotBands) < 3){
                  orig_scale_im_raster <-  (as.array(ds_next_in[1,,,plotBands[1]]))
                  causalimages::image2(
                    as.matrix( orig_scale_im_raster ),
                    main = main_, cex.main = 4,
                    cex.lab = 2.5, col.lab = k_, col.main = k_,
                    xlab = ifelse(!is.null(long),
                                  yes = sprintf("Long: %s, Lat: %s",
                                                fixZeroEndings(round(coordinate_i,2L)[1],2L),
                                                fixZeroEndings(round(coordinate_i,2L)[2],2L)),
                                  no = ""))
              }
              if(length(plotBands) >= 3){
                orig_scale_im_raster <- raster::brick( 0.0001 +
                    0*runif(length(as.array(ds_next_in[1, , ,plotBands])), min = 0, max = 0.01) + # random jitter
                    (as.array(ds_next_in[1,,,plotBands])) )
                stretch <- ifelse(
                    any(apply(as.array(ds_next_in[1, , ,plotBands]), 3,sd) < 1.),
                            yes = "", no = "lin")
                #raster::plotRGB(raster::brick(abs(tmp)),stretch="")
                # raster::plotRGB(  orig_scale_im_raster, stretch = "lin")
                raster::plotRGB(  orig_scale_im_raster,
                                 margins = T,
                                 r = 1, g = 2, b = 3,
                                 mar = (margins_vec <- (ep_<-1e-6)*c(1,3,1,1)),
                                 main = main_,
                                 cex.lab = 2.5, col.lab = k_,
                                 xlab = ifelse(!is.null(long),
                                           yes = sprintf("Long: %s, Lat: %s",
                                                fixZeroEndings(round(coordinate_i,2L)[1],2L),
                                                fixZeroEndings(round(coordinate_i,2L)[2],2L)),
                                           no = ""),
                                 col.main = k_, cex.main=4,  stretch = stretch)
              }
              if(grepl(typePlot,pattern = "mean")){
                # axis for plot
                ylab_ <- ""; if(i==1){
                  tauk <- eval(parse(text = sprintf("tau%s",k_)))
                  ylab_ <- eval(parse(text = sprintf("expression(hat(tau)[%s]==%.3f)",k_,tauk)))
                  if(orthogonalize == T){
                    ylab_ <- eval(parse(text = sprintf("expression(hat(tau)[%s]^{phantom() ~ symbol('\136') ~ phantom()}==%.3f)",k_, tauk)))
                  }
                  axis(side = 2,at=0.5,labels = ylab_,pos=-0.,tick=F,cex.axis=cex_tile_axis <- 4,
                       col.axis=k_)
                }

                #obtain image gradients
                {
                  take_k <- k_
                  if(i == 1){
                    ImageGrad_fxn <- (function(m){
                      m <- tf$Variable(m,trainable = T)
                      with(tf$GradientTape(watch_accessed_variables = F,persistent  = T) %as% tape, {
                        tape$watch( m )
                        PROBS_ <- tf$reduce_mean(tf$concat(
                          replicate(nMonte_salience, getClusterProb(m,training = F)),0L),0L)
                        PROBS_Smoothed <- tf$add(tf$multiply(tf$subtract(tf$constant(1), ep_LabelSmooth<-tf$constant(0.01)),PROBS_),
                                                tf$divide(ep_LabelSmooth,tf$constant(2)))
                        #OUTPUT_ <- LOGIT_ <- tf$subtract(tf$math$log(PROBS_Smoothed), tf$math$log(tf$subtract(tf$constant(1), PROBS_Smoothed) ))
                        OUTPUT_ <- LOG_PROBS_ <- tf$math$log(PROBS_Smoothed)
                      })
                      ImageGrad <- tape$jacobian( OUTPUT_, m , experimental_use_pfor = F)
                      ImageGrad_o <- tf$gather(ImageGrad, indices = as.integer(take_k-1L), axis = 0L)
                      for(jf in 1:2){
                        if(jf == 1){ImageGrad <- tf$math$reduce_euclidean_norm(ImageGrad_o+0.0000001, axis = 3L, keepdims = T)}
                        if(jf == 2){ImageGrad <- tf$math$reduce_mean(ImageGrad_o, axis = 3L, keepdims = T)}
                        ImageGrad <- tf$gather(AveragingConv(ImageGrad),0L,axis = 0L)
                        if(jf == 1){ImageGrad_L2 <- ImageGrad}
                        if(jf == 2){ImageGrad_E <- ImageGrad}
                      }
                      return(tf$concat(list(ImageGrad_L2,ImageGrad_E),2L))
                    })
                    AveragingConv <- tf$keras$layers$Conv2D(filters=1L,
                                                            kernel_size = gradAnalysisFilterDim <- 10L,
                                                            padding = "valid")
                    AveragingConv( tf$expand_dims(tf$gather(ds_next_in,1L, axis = 3L),3L)  )
                    AveragingConv$trainable_variables[[1]]$assign( 1 / gradAnalysisFilterDim^2 *tf$ones(tf$shape(AveragingConv$trainable_variables[[1]])) )
                  }
                  IG <- as.array( ImageGrad_fxn( InitImageProcess(ds_next_in, training = F)  ))

                  { #if(i == 1){
                    nColors <- 1000
                    # pos/neg breaks should be on the same scale across observation
                    IG_forBreaks <- IG + runif(length(IG),-0.0000000000001, 0.0000000000001)
                    pos_breaks <- try(sort( quantile(c(IG_forBreaks[,,2][IG_forBreaks[,,2]>=0]),probs = seq(0,1,length.out=nColors/2),na.rm=T)),T)
                    if("try-error" %in% class(pos_breaks)){pos_breaks <-  seq(0, 1,length.out=nColors/2) }

                    neg_breaks <- try(sort(quantile(c(IG_forBreaks[,,2][IG_forBreaks[,,2]<=0]),probs = seq(0,1,length.out=nColors/2),na.rm=T)),T)
                    if("try-error" %in% class(neg_breaks)){neg_breaks <-  seq(-1, 0,length.out=nColors/2) }

                    gradMag_breaks <- try(sort(quantile((c(IG_forBreaks[,,1])),probs = seq(0,1,length.out = nColors),na.rm=T)),T)
                    if("try-error" %in% class(gradMag_breaks)){gradMag_breaks <-  seq(-1, 1,length.out=nColors) }
                  }

                  # magnitude
                  # try(print2(summary(c( IG[,,1] ))), T)
                  magPlot <- image(t(IG[,,1])[,nrow(IG[,,1]):1],
                            col = viridis::magma(nColors - 1),
                            breaks = gradMag_breaks, axes = F)
                  if("try-error" %in% class(magPlot)){
                    print2("magPlot broken")
                  }
                  ylab_ <- ""; if(i==1){
                    try(axis(side = 2,at=0.5,labels = "Salience Magnitude",
                         pos=-0.,tick=F, cex.axis=3, col.axis=k_),T)
                  }

                  # direction
                  dirPlot <- image(t(IG[,,2])[,nrow(IG[,,2]):1],
                            col = c(hcl.colors(nColors/2-1L,"reds"),
                                    hcl.colors(nColors/2 ,"blues")),
                            breaks = c(neg_breaks,pos_breaks), axes = F)
                  if("try-error" %in% class(dirPlot)){print2("dirPlot broken")}
                  ylab_ <- ""; if(i==1){
                    try( axis(side = 2,at=0.5,labels = "Salience Direction",
                            pos=-0.,tick=F, cex.axis=3, col.axis=k_),  T)
                  }
                }
              }
            }
            plotting_coordinates_mat <- try(rbind(plotting_coordinates_mat,
                                                  used_coordinates ),T)
            if("try-error" %in% class(plotting_coordinates_mat)){browser()}
            print2(used_coordinates)
          }
        }
        dev.off()
        return( plotting_coordinates_mat )
        }
      }

      if(CausalImagesDataType == "video"){
        plot_fxn <- function(){
          {
            plotting_coordinates_mat <- c()
            total_counter <- 0; ep_LabelSmooth <- tf$constant(0.01)
            for(k_ in 1:rows_){
              used_coordinates <- c()
              for(i in 1:5){
                #if(k_ == 2 & typePlot == "mean"){ browser() }
                #if(k_ == 2 & i == 1){ browser() }
                print2(sprintf("Type Plot: %s; k_: %s, i: %s", typePlot, k_, i))
                total_counter <- total_counter + 1
                rfxn <- function(xer){xer}
                bad_counter <- 0;isUnique_ <- F; while(isUnique_ == F){
                  BreakTies <- function(x){x + runif(length(x),-1e-3,1e-3)}
                  if(typePlot == "uncertainty"){
                    main_ <- letters[  total_counter  ]

                    # plot images with largest std's
                    valBrokenTies <- BreakTies(ClusterProbs_std[,k_])
                    sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                    im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                  }
                  if(grepl(typePlot,pattern = "mean")){
                    main_ <- total_counter

                    # plot images with largest lower confidence
                    if(typePlot ==  "mean"){
                      valBrokenTies <- BreakTies(ClusterProbs_lower_conf[,k_])
                      sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                      im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                    }

                    # plot images with largest cluster probs
                    if(typePlot ==  "mean_upperConf"){
                      valBrokenTies <- BreakTies(ClusterProbs_est_full[,k_])
                      sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                      im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                    }
                  }

                  coordinate_i <- c(long[im_i], lat[im_i])
                  if(bad_counter>50){browser()}
                  if(i > 1){
                    isUnique_ <- F; if(!is.null(long)){
                      dist_m <- geosphere::distm(coordinate_i,
                                                 used_coordinates,
                                                 fun = geosphere::distHaversine)
                      bad_counter <- bad_counter + 1
                      if(all(dist_m >= 1000)){isUnique_ <- T}
                    } }
                  if(i == 1){ isUnique_<-T }
                }

                used_coordinates <- rbind(coordinate_i,used_coordinates)
                print2(c(k_, i, im_i, long[im_i], lat[im_i]))

                # load in video
                if(acquireImageMethod == "functional"){
                  ds_next_in <- acquireImageFxn(  transportabilityMat$key[im_i], training = F )
                }
                if(acquireImageMethod == "tf_record"){
                  #setwd(orig_wd)
                  ds_next_in <- GetElementFromTfRecordAtIndices( indices = im_i,
                                                                 filename = file,
                                                                 readVideo = useVideo,
                                                                 nObs = length(imageKeysOfUnits) )
                  #setwd(new_wd)
                  ds_next_in <- ds_next_in[[1]]
                  if(length(ds_next_in$shape) == 4){ ds_next_in <- tf$expand_dims(ds_next_in, 0L) }
                }


                if(length(plotBands) < 3){
                  orig_scale_im_raster <-  (as.array(ds_next_in[1,,,,plotBands[1]]))
                  animation::saveGIF({
                    for (t_ in 1:nTimeSteps) {
                      causalimages::image2(
                        as.matrix( orig_scale_im_raster[t_,,] ),
                        main = main_, cex.main = 4,
                        cex.lab = 2.5, col.lab = k_, col.main = k_,
                        xlab = ifelse(!is.null(long),
                                      yes = sprintf("Long: %s, Lat: %s",
                                                    fixZeroEndings(round(coordinate_i,2L)[1],2L),
                                                    fixZeroEndings(round(coordinate_i,2L)[2],2L)),
                                      no = ""))
                      animation::ani.pause()  # Pause to make sure it gets rendered
                  } }, movie.name = sprintf("%s/HeteroSimClusterEx%s_ExternalFigureKey%s_k%s_i%s.gif",
                                          figuresPath, pdf_name_key, figuresTag, k_, i),
                       autobrowse = F, autoplay = F)
                }
                if(length(plotBands) >= 3){
                  animation::saveGIF({
                    for(t_ in 1:nTimeSteps){
                    print( im_i )
                    orig_scale_im_raster <- raster::brick(
                      0.0001 + (as.array(ds_next_in[1,t_, , ,plotBands])) +
                      0*runif(length(as.array(ds_next_in[1,t_, , ,plotBands])), min = 0, max = 0.01) # random jitter
                    )
                    stretch <- ifelse(
                      any(apply(as.array(ds_next_in[1,t_, , ,plotBands]), 3,sd) < 1.),
                      yes = "", no = "lin")
                    # raster::plotRGB(  orig_scale_im_raster, stretch = "lin")
                    raster::plotRGB(  orig_scale_im_raster,
                                      margins = T,
                                      r = 1, g = 2, b = 3,
                                      mar = (margins_vec <- (ep_<-1e-6)*c(1,3,1,1)),
                                      main = main_,
                                      cex.lab = 2.5, col.lab = k_,
                                      xlab = ifelse(!is.null(long),
                                                    yes = sprintf("Long: %s, Lat: %s",
                                                                  fixZeroEndings(round(coordinate_i,2L)[1],2L),
                                                                  fixZeroEndings(round(coordinate_i,2L)[2],2L)),
                                                    no = ""),
                                      col.main = k_, cex.main=4,  stretch = stretch)
                    }}, movie.name = sprintf("%s/HeteroSimClusterEx%s_ExternalFigureKey%s_k%s_i%s.gif",
                                               figuresPath, pdf_name_key, figuresTag, k_, i) )
                }
                if(grepl(typePlot,pattern = "mean")){
                  # axis for plot
                  ylab_ <- ""; if(i==1){
                    tauk <- eval(parse(text = sprintf("tau%s",k_)))
                    ylab_ <- eval(parse(text = sprintf("expression(hat(tau)[%s]==%.3f)",k_,tauk)))
                    if(orthogonalize == T){
                      ylab_ <- eval(parse(text = sprintf("expression(hat(tau)[%s]^{phantom() ~ symbol('\136') ~ phantom()}==%.3f)",k_, tauk)))
                    }
                  }

                  #obtain image gradients
                  {
                    take_k <- k_
                    if(i == 1){
                      # don't jit -- take_k dynamic within
                      ImageGrad_fxn <- (function(m){
                        m <- tf$Variable(m, trainable = T)
                        with(tf$GradientTape(watch_accessed_variables = F,persistent  = T) %as% tape, {
                          tape$watch( m )
                          PROBS_ <- tf$reduce_mean(tf$concat(
                            replicate(nMonte_salience, getClusterProb(m, training = F)),0L),0L)
                          PROBS_Smoothed <- tf$add(tf$multiply(tf$subtract(tf$constant(1), ep_LabelSmooth),PROBS_),
                                                   tf$divide(ep_LabelSmooth,tf$constant(2)))
                          #OUTPUT_ <- LOGIT_ <- tf$subtract(tf$math$log(PROBS_Smoothed), tf$math$log(tf$subtract(tf$constant(1), PROBS_Smoothed) ))
                          OUTPUT_ <- LOG_PROBS_ <- tf$math$log(PROBS_Smoothed)
                        })
                        ImageGrad <- tape$jacobian( OUTPUT_, m , experimental_use_pfor = F)
                        ImageGrad_o <- tf$gather(ImageGrad, indices = as.integer(take_k-1L), axis = 0L)
                        print(dim(ImageGrad_o))
                        for(jf in 1:2){
                          if(jf == 1){ImageGrad <- tf$math$reduce_euclidean_norm(ImageGrad_o+0.0000001, 4L,keepdims = T)}
                          if(jf == 2){ImageGrad <- tf$math$reduce_mean(ImageGrad_o, 4L, keepdims = T)}

                          # uncomment if seeking to average
                          #ImageGrad <- tf$gather(AveragingConv(ImageGrad),0L,axis = 0L)
                          if(jf == 1){ImageGrad_L2 <- ImageGrad}
                          if(jf == 2){ImageGrad_E <- ImageGrad}
                        }
                        return(tf$concat(list(ImageGrad_L2,  # salience magnitude
                                              ImageGrad_E), # salience direction
                                         4L))
                      })
                      AveragingConv <- tf$keras$layers$Conv3D(filters=12L,
                                                              kernel_size = c(2L, (gradAnalysisFilterDim<-10L), 10L),
                                                              padding = "valid")
                      AveragingConv( tf$expand_dims(tf$gather(
                        InitImageProcess(ds_next_in,training = F),
                                         1L, axis = 4L),4L)  )
                      AveragingConv$trainable_variables[[1]]$assign( 1 / gradAnalysisFilterDim^2 *tf$ones(tf$shape(AveragingConv$trainable_variables[[1]])) )
                      # AveragingConv$trainable_variables[[1]]$shape
                    }
                    IG <- as.array( ImageGrad_fxn(
                          m = InitImageProcess(ds_next_in, training = F)))[1,,,,]

                    # check for temporal symmetry
                    #image2(as.array(ds_next_in)[1,1,,,1])
                    #image2(as.array(ds_next_in)[1,2,,,1])
                    #image2(IG[1,,,1])
                    #image2(IG[2,,,1])
                    #image2(as.array(InitImageProcess( acquireImageFxn( imageKeysOfUnits[im_i], training = F), training = F))[1,1,,,1])
                    #image2(as.array(InitImageProcess( acquireImageFxn( imageKeysOfUnits[im_i], training = F), training = F))[1,2,,,1])
                    #image2(IG[1,,,1])
                    #image2(IG[2,,,1])

                    nColors <- 1000
                    { #if(i == 1){
                      # pos/neg breaks should be on the same scale across observation
                      gradMag_breaks <- try(sort(quantile((c(IG[,,,1])),probs = seq(0,1,length.out = nColors),na.rm=T)),T)
                      if("try-error" %in% class(gradMag_breaks)){gradMag_breaks <-  seq(-1, 1,length.out = nColors) }
                    }

                    # magnitude - check for changes in salings
                    #axis(side = 2,at=0.5,labels = ylab_,pos=-0.,tick=F,cex.axis=cex_tile_axis <- 4,col.axis=k_)
                    print2(summary(c( IG[,,,1] )))
                    animation::saveGIF({
                      for (t_ in 1:nTimeSteps) {
                         par(mar = c(1,5,1,1))
                         try(image(t(IG[t_,,,1])[,nrow(IG[t_,,,1]):1],
                                         col = viridis::magma(nColors - 1),
                                         ylab = "Salience Magnitude",cex.axis = 3,
                                         cex.axis=(cex_tile_axis <- 4), col.axis=k_,
                                         breaks = gradMag_breaks, axes = F),T)
                        animation::ani.pause()  # Pause to make sure it gets rendered
                      }},movie.name = sprintf("%s/HeteroSimClusterEx%s_ExternalFigureKey%s_k%s_i%s_SalienceMag.gif",
                                              figuresPath, pdf_name_key, figuresTag, k_, i),
                          autobrowse = F, autoplay = F)
                  }
                }
              }
              plotting_coordinates_mat <- try(rbind(plotting_coordinates_mat,
                                                    c("observation_index"=im_i, used_coordinates)),T)
              if("try-error" %in% class(plotting_coordinates_mat)){browser()}
              print2(used_coordinates)
            }
          }
          return( plotting_coordinates_mat )
        }
      }

      plotting_coordinates_mat_ <- try(plot_fxn(),T)

      # tests
      if(T == F){
        # table(UgandaDataProcessed_$geo_long_lat_key[plotting_coordinates_mat_[,"observation_index"]])
        # table(imageKeysOfUnits[plotting_coordinates_mat_[,"observation_index"]])
        ds_next_in <- GetElementFromTfRecordAtIndices( indices = as.integer(plotting_coordinates_mat_[,1]),
                                                       filename = file,
                                                       readVideo = useVideo,
                                                       nObs = length(imageKeysOfUnits) )
        imageKeysOfUnits[plotting_coordinates_mat_[,"observation_index"]]
        ds_next_in[[3]]
        image2(as.array(ds_next_in[[1]])[1,,,1])
        image2(as.array(ds_next_in[[1]])[2,,,1])
        image2(as.array(ds_next_in[[1]])[3,,,1])
      }
      try(dev.off(),T)
      if("try-error" %in% class(plotting_coordinates_mat_)){ browser() }
      plotting_coordinates_list[[typePlot_counter]] <- plotting_coordinates_mat_
    }
    try({ names(plotting_coordinates_list) <- typePlot_vec},T)
    par(mfrow=c(1,1))

    return( list(
                 "clusterTaus_mean" = as.numeric(tau_vec),
                 "clusterTaus_sd" = Tau_sd_vec,
                 "clusterProbs_mean" = ClusterProbs_est_full,
                 "clusterProbs_sd" = ClusterProbs_std,
                 "clusterProbs_lowerConf" = ClusterProbs_lower_conf,
                 "impliedATE" = impliedATE,
                 "individualTau_est" = tau_i_est,
                 "transportabilityMat" = transportabilityMat,
                 "plottedCoordinatesList" = plotting_coordinates_list,
                 "whichNA_dropped" = whichNA_dropped) )
  }
  if(simMode == T){
    # depreciated
    return(list(
                "ClusterProbs_mean" = ClusterProbs_est,
                "ClusterProbs" = ClusterProbs,
                "Cluster" = as.numeric(tau1),
                "tau2" = as.numeric(tau2),
                "sd_tau1" = sd_tau1,
                "sd_tau2" = sd_tau2,
                #"R2_Y0"=r2_y0_out,
                #"R2_Y1"=r2_y1_out,
                "tau_i_est"=tau_i_est,
                "impliedATE" = impliedATE,
                #"y0_true_out" = y0_true,
                #"y1_true_out" = y1_true,
                #"y0_est_out" = y0_est,
                #"y1_est_out" = y1_est,
                #"negELL"=as.numeric(negELL),
                "whichNA_dropped" = whichNA_dropped
                ))
  }
}
