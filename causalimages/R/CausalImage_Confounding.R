#!/usr/bin/env Rscript
#' Perform causal estimation under image confounding
#'
#' @usage
#'
#' AnalyzeImageConfounding(obsW, obsY, imageKeysOfUnits, acquireImageFxn, ...)
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param acquireImageFxn A function specifying how to load images representations associated with `imageKeysOfUnits` into memory. For example, if observation `3` has a value  of `"a34f"` in `imageKeysOfUnits`, `acquireImageFxn` should extract the image associated with the unique key `"a34f"`.
#' First argument should be image key values and second argument have be `training` (in case different behavior in training/inference mode).
#' @param transportabilityMat (optional) A matrix with a column named `imageKeysOfUnits` specifying keys to be used by `acquireImageFxn` for generating treatment effect predictions for out-of-sample points.
#' @param imageKeysOfUnits (default = `1:length(obsY)`) A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into `acquireImageFxn` to call images into memory.
#' @param long,lat (optional) Vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.
#' @param X (optional) A numeric matrix containing tabular information used if `orthogonalize = T`. `X` is normalized internally and salience maps with respect to `X` are transformed back to the original scale.
#' @param file (default = `NULL`) Path to a tfrecord file generated by `WriteTfRecord`.
#' @param conda_env (default = `NULL`) A string specifying a conda environment wherein `tensorflow`, `tensorflow_probability`, and `gc` are installed.
#' @param conda_env_required (default = `F`) A Boolean stating whether use of the specified conda environment is required.
#' @param figuresTag (default = `""`) A string specifying an identifier that is appended to all figure names.
#' @param tagInFigures (default = `F`) A Boolean specifying whether to visually include the tag in the figures.
#' @param figuresPath (default = `"./"`) A string specifying file path for saved figures made in the analysis.
#' @param plotBands (default = `1L`) An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RBG plotting).
#' @param kernelSize (default = `5L`) Dimensions used in convolution kernels.
#' @param nSGD (default = `400L`) Number of stochastic gradient descent (SGD) iterations.
#' @param nBoot (default = `100L`) Number of bootstrap iterations for uncertainty estimation.
#' @param typeBoot (default = `SamplingOnly`) Bootstrap type. `typeBoot = 'SamplingOnly'` captures sampling uncertainty only. `typeBoot = 'EstimationAndSampling'` captures both estimation and sampling uncertainty. `typeBoot = 'InitializationEstimationAndSampling'` captures initializatoin, estimation, and sampling uncertainty.
#' @param batchSize (default = `50L`) Batch size used in SGD optimization.
#' @param doConvLowerDimProj (default = `T`) Should we project the `nFilters` convolutional feature dimensions down to `nDimLowerDimConv` to reduce the number of required parameters.
#' @param nDimLowerDimConv (default = `3L`) If `doConvLowerDimProj = T`, then, in each convolutional layer, we project the `nFilters` feature dimensions down to `nDimLowerDimConv` to reduce the number of parameters needed.
#' @param nFilters (default = `50L`) Integer specifying the number of convolutional filters used.
#' @param nEmbedDim (default = `96L`) Integer specifying the image/image sequence embedding dimension. Used if `modelClass = "embeddings"`.
#' @param nDenseWidth (default = `32L`) Width of dense projection layers post-convolutions.
#' @param dropoutRate (default = `0.1`) Droppout rate used in training used to prevent overfitting (`dropoutRate = 0` corresponds to no dropout).
#' @param testFrac (default = `0.01`) Fraction of observations held out as a test set to evaluate out-of-sample loss values.
#' @param nDepthHidden_conv (default = `3L`) Hidden depth of convolutional layer.
#' @param nDepthHidden_dense (default = `0L`) Hidden depth of dense layers. Default of `0L` means a single projection layer is performed after the convolutional layer (i.e., no hidden layers are used).
#' @param quiet (default = `F`) Should we suppress information about progress?
#' @param maxPoolSize (default = `2L`) Integer specifying the max pooling size used in the convolutional layers.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.
#' @param simMode (default = `F`) Should the analysis be performed in comparison with ground truth from simulation?
#' @param tf_seed (default = `NULL`) Specification for the tensorflow seed.
#' @param modelClass (default = `"cnn"`) Either `"cnn"` or `"embeddings"`.
#' @param plotResults (default = `T`) Should analysis results be plotted?
#' @param channelNormalize (default = `T`) Should channelwise image feature normalization be attempted? Default is `T`, as this improves training.
#' @param TfRecords_BufferScaler (default = `4L`) The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#'
#' @return Returns a list consisting of
#' \itemize{
#'   \item `ATE_est` ATE estimate.
#'   \item `ATE_se` Standard error estimate for the ATE.
#'   \item `plotResults` If set to `TRUE`, causal salience plots are saved to disk, characterizing the image confounding structure. See references for details.
#' }
#'
#' @section References:
#' \itemize{
#' \item  Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Integrating Earth Observation Data into Causal Inference: Challenges and Opportunities. *ArXiv Preprint*, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

AnalyzeImageConfounding <- function(
                                   obsW,
                                   obsY,
                                   X = NULL,
                                   file = NULL,
                                   imageKeysOfUnits = NULL,
                                   doConvLowerDimProj = T,
                                   nDimLowerDimConv = 3L,
                                   nFilters = 50L,
                                   samplingType = "none",
                                   modelClass = "embeddings",
                                   nBoot = 50L,
                                   typeBoot = "SamplingOnly",
                                   nEmbedDim = 96L,
                                   HiddenDim  = 64L,
                                   DenseActivation = "linear",
                                   inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
                                   useTrainingPertubations = T,

                                   orthogonalize = F,
                                   acquireImageFxn = NULL ,
                                   transportabilityMat = NULL ,
                                   lat = NULL,
                                   long = NULL,
                                   conda_env = NULL,
                                   conda_env_required = F,

                                   figuresTag = "",
                                   figuresPath = "./",
                                   tagInFigures = F,
                                   plotBands = 1L,

                                   simMode = F,
                                   plotResults = T,

                                   nDepthHidden_conv = 1L,
                                   nDepthHidden_dense = 0L,
                                   maxPoolSize = 2L,
                                   strides = 1L,
                                   compile = T,
                                   dropoutRate = 0.1,
                                   batchSize = 50L,
                                   kernelSize = 3L,
                                   temporalKernelSize = 2L,
                                   nSGD  = 400L,
                                   testFrac = 0.01,
                                   nDenseWidth = 32L,
                                   channelNormalize = T,
                                   printDiagnostics = F,
                                   TfRecords_BufferScaler = 4L,
                                   LEARNING_RATE_BASE = 0.005,
                                   dataType = "image",
                                   tf_seed = NULL,
                                   quiet = F){
  print("Initializing the tensorflow environment...")
  print("Looking for Python modules tensorflow, gc...")
  {
    library(tensorflow); library(keras)
    if(!is.null(conda_env)){
      try(tensorflow::use_condaenv(conda_env, required = conda_env_required),T)
    }
    Sys.sleep(1.); try(tf$square(1.),T); Sys.sleep(1.)
    try(tf$config$experimental$set_memory_growth(tf$config$list_physical_devices('GPU')[[1]],T),T)
    try( tf$config$set_soft_device_placement( T ) , T)
    #tfd <- (tfp <- tf_probability())$distributions
    #tfa <- reticulate::import("tensorflow_addons")

    # setting seed can cause good or bad effects
    # https://github.com/tensorflow/tensorflow/issues/37252
    try(tf$random$set_seed(  c( ifelse(is.null(tf_seed),yes = 123431L, no = as.integer(tf_seed)  ) )), T)
    try(tf$keras$utils$set_random_seed( c( ifelse(is.null(tf_seed), yes = 123419L, no = as.integer(tf_seed)  ) )), T)

    # import python garbage collectors
    py_gc <- reticulate::import("gc")
    gc(); py_gc$collect()

    # define tf function
    # remember - inputs to tf_functions should be tf$constants
    tf_function_use  <- tf_function
    #tf_function_use  <- function(x){print("Warning: Not using tf_function()"); return(x)}
  }

  # make all directory logic explicit
  orig_wd <- getwd()
  cond1 <- substr(figuresPath, start = 0, stop = 1) == "."
  cond2 <- substr(figuresPath, start = 0, stop = 1) == "/"
  if(cond1){
    figuresPath <- gsub(figuresPath, pattern = '\\.', replace = orig_wd)
  }

  if(is.null(imageKeysOfUnits) & !is.null(imageKeysOfUnits)){ imageKeysOfUnits <- keys }
  if(batchSize > length(obsW)){ batchSize <- round(length(obsW) * 0.90) }

  if(!is.null(X)){ if(!"matrix" %in% class(X)){
    print("Coercing X to matrix class...")
    X <- as.matrix(  X )
  } }

  if(!is.null(X)){ if(is.na(sum(X))){
    stop("Error: is.na(sum(X)) is TRUE; check for NAs or that all variables are numeric.")
  }}

  if(!is.null(X)){ if(any(apply(X,2,sd) == 0)){
    stop("Error: any(apply(X,2,sd) == 0) is TRUE; a column in X seems to have no variance; drop column!")
  }}

  #if(!is.null(X)){ if( abs(mean(apply(X,2,sd))-1)>0.01 | abs(mean(apply(X,2,mean))-0)>0.01){print("Note: We noticed that X is not normalized. Normalizing X (to mean 0, sd = 1) is recommended!...") }}
  if(!is.null(X)){
    X_mean <- colMeans(X)
    X_sd <- apply(X,2,sd)
    X <- t( (t(X) - X_mean ) / (0.00001+X_sd) )
  }

  XisNull <- F; if( is.null(X) ){
    XisNull <- T; X <- as.matrix( rnorm(length(obsW), sd = 0.01 ) )
  }

  {
    loss_vec <- NULL
    acquireImageMethod <- "functional";
    # define base tf record + train/test fxns
    changed_wd <- F; if(  !is.null(  file  )  ){
      acquireImageMethod <- "tf_record"

      # established tfrecord connection
      print("Establishing connection with tfrecord")
      tf_record_name <- file
      if( !grepl(tf_record_name, pattern = "/") ){
        tf_record_name <- paste("./",tf_record_name, sep = "")
      }
      tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
      new_wd <- paste(tf_record_name[-length(tf_record_name)], collapse = "/")
      print( sprintf("Temporarily re-setting the wd to %s", new_wd ) )
      changed_wd <- T; setwd( new_wd )
      tf_dataset <- tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

      # helper functions
      useVideo <- dataType == "video"
      getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
        return( dataset <- dataset$batch( as.integer(max(2L,round(batchSize/2L)  ))) )
      }

      getParsed_tf_dataset_train <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
                                   #num_parallel_calls = tf$data$AUTOTUNE)
        dataset <- dataset$shuffle(buffer_size = tf$constant(as.integer(TfRecords_BufferScaler*batchSize),dtype=tf$int64),
                                   reshuffle_each_iteration = T)
        dataset <- dataset$batch(  as.integer(batchSize)   )
        #dataset <- dataset$prefetch(tf$data$AUTOTUNE)
        return( dataset  )
      }

      # setup iterators
      #Sys.setenv(M_CHECK_ = "1")
      #LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
      #Sys.setenv(MALLOC_TRIM_THRESHOLD_ = "0")
      tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
      #iterator = dataset.shuffle(int(1e7)).batch(int(1e6)).repeat(10)
      #tf_dataset_train <- tf_dataset_train$`repeat`(  as.integer(2*ceiling(batchSize*nSGD / length(obsY)  ) ) )
      tf_dataset_train <- tf_dataset_train$`repeat`(  -1L )
      tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

      # reset iterators
      ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
      if(T == F){
        # tests; see https://stackoverflow.com/questions/72552605/how-to-fix-tensorflow-datasets-memory-leak-when-shuffling
        ds_next_train <- reticulate::iter_next( ds_iterator_train )
        batch_indices <- as.array(ds_next_train[[2]])
      }
      ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )

      # checks
      # ds_iterator_inference$output_shapes; ds_iterator_train$output_shapes
      # ds_next_train <- reticulate::iter_next( ds_iterator_train )
      # ds_next_inference <- reticulate::iter_next( ds_iterator_inference )
    }

    trainingPertubations <- tf$identity
    #iterationFxn <- function(x){ x %% 4 }
    iterationFxn <- function(x){ sample(0L:3L,1) }
    if(useTrainingPertubations){
      #flip_left_right <- (function(x){ tf$image$flip_left_right(x)})
      #flip_up_down <- (function(x){ tf$image$flip_up_down(x)})
      flip_left_right <- tf_function_use(function(x){ tf$image$flip_left_right(x)})
      flip_up_down <- tf_function_use(function(x){ tf$image$flip_up_down(x)})
      #trainingPertubations <- tf_function_use(function(im__, iteration){
      trainingPertubations <- (function(im__, iteration){
        #with( tf$device('/CPU:0'), {
        ({

          if(iteration == 0){
            # do nothing
          }
          if(iteration == 1){
            #im__ <- tf$image$flip_left_right(im__)
            im__ <- flip_left_right(im__)
          }
          if(iteration == 2){
            #im__ <- tf$image$flip_up_down(im__)
            im__ <- flip_up_down(im__)
          }
          if(iteration == 3){
            #im__ <- tf$image$flip_left_right( tf$image$flip_up_down(im__) )
            im__ <- flip_left_right( flip_up_down(im__) )
          }
          #im__ <- tf$image$random_flip_left_right(im__, iteration)
          #im__ <- tf$image$random_flip_up_down(im__, iteration)
          #im__ <- tf$image$random_saturation(im__, 5, 10)
          #im__ <- tf$image$adjust_brightness(im__, 0.1)
          return( im__ )
        })
    })
    }

    binaryCrossLoss <- function(W,prW){return( - mean( log(prW+0.001)*W + log(1-prW+0.001)*(1-W) ) ) }
    InitImageProcess <- tf_function_use(function(im,
                                                 training,
                                                 iteration){

      # expand dims if needed
      if(length(imageKeysOfUnits) == 1){ im <- tf$expand_dims(im,0L) }

      # normalize
      im <- tf$divide(tf$subtract(im, NORM_MEAN_array), NORM_SD_array)

      # training pertubations if desired
      # note: trainingPertubations must be performed on CPU
      if(training == T){ im <- trainingPertubations(im, iteration) }

      # downshift resolution if desired
      if(inputAvePoolingSize > 1){ im <- AvePoolingDownshift(im) }
      return( im  )
    })

    # some hyperparameters parameters
    figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")
    KernalActivation <- "swish"
    KernalProjActivation <- "swish"
    HiddenActivation <- "swish"
    BN_MOMENTUM <- 0.90
    poolingAt <- 1L # do pooling every poolingAt iterations
    poolingBy <- 2L # pool by poolingBy by poolingBy
    poolingType <- "max"
    widthCycle <- 50
    doParallel <- F

    # get first iter batch for initializations
    print("Calibrating first moments for input data normalization...")
    NORM_SD <- NORM_MEAN <- c()
    for(momentCalIter in 1:(momentCalIters<-10)){
      if(acquireImageMethod == "tf_record"){
        ds_next_train <- reticulate::iter_next( ds_iterator_train )
        batch_indices <- as.array(ds_next_train[[2]])
      }
      if(acquireImageMethod == "functional"){
        batch_indices <- sample(1:length(obsY),batchSize,replace = F)
        ds_next_train <- list(
          r2const( acquireImageFxn(imageKeysOfUnits[batch_indices], training = F) , dtype = tf$float32 )
        )
      }

      # setup normalizations
      if(is.null(NORM_MEAN)){
        NORM_MEAN <- NORM_SD <- apply(as.array(ds_next_train[[1]]),4,sd)
        NORM_MEAN[] <- NORM_SD[] <- 0
      }

      # update normalizations
      NORM_SD <- NORM_SD + apply(as.array(ds_next_train[[1]]),4,sd) / momentCalIters
      NORM_MEAN <- NORM_MEAN + apply(as.array(ds_next_train[[1]]),4,mean) / momentCalIters
    }
    NORM_MEAN_array <- tf$constant(array(NORM_MEAN,dim=c(1,1,1,length(NORM_MEAN))),tf$float32)
    NORM_SD_array <- tf$constant(array(NORM_SD,dim=c(1,1,1,length(NORM_SD))),tf$float32)
    py_gc$collect()

    # define train/test indices
    testIndices <- sample(1:length(obsY), max(2,length(obsY)*testFrac))
    trainIndices <- (1:length(obsY))[! 1:length(obsY) %in% testIndices]

    # define image downshift for supplementary analyses
    AvePoolingDownshift <- tf$keras$layers$AveragePooling2D(pool_size = as.integer(c(inputAvePoolingSize,inputAvePoolingSize)))

    # set up holders
    prW_est <- rep(NA,times = length(obsW))

    # initialize layers
    if(modelClass == "cnn"){
    print( "Initializing CNN layers..." )
    try(eval(parse(text = paste("rm(", paste(trainable_layers,collapse=","),")"))),T)
    trainable_layers <- ls()
    {
      GlobalMaxPoolLayer <- tf$keras$layers$GlobalMaxPool2D(data_format="channels_last",name="GlobalMax")
      GlobalAvePoolLayer <- tf$keras$layers$GlobalAveragePooling2D(data_format="channels_last",name="GlobalAve")
      GlobalPoolLayer <- function(z){
        return(tf$concat(list(GlobalMaxPoolLayer(z),GlobalAvePoolLayer(z)),1L)) }
      BNLayer_Axis1_inputDense <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOMENTUM, epsilon = 0.001)
      #BNLayer_Axis1_final <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOMENTUM, epsilon = 0.001)
      if(nDepthHidden_dense > 0){ for(jr in 1:nDepthHidden_dense){
        eval(parse(text = sprintf('HiddenProjection%s <- tf$keras$layers$Dense(HiddenDim, activation = "linear")', jr)))
        eval(parse(text = sprintf('BNLayer_Axis1_hidden%s <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOMENTUM, epsilon = 0.001)', jr)))
      }}
      DenseLayer <- tf$keras$layers$Dense(1L, activation = DenseActivation)
      FlattenLayer <- tf$keras$layers$Flatten(data_format = "channels_last")

      BNLayer_Axis3_init <- tf$keras$layers$BatchNormalization(axis = 3L, center = F, scale = F, momentum = BN_MOMENTUM, epsilon = 0.001,name="InitNorm")
      for(d_ in 1:nDepthHidden_conv){
        eval(parse(text = sprintf('Conv%s <- tf$keras$layers$Conv2D(filters=nFilters,
                                  kernel_size=c(kernelSize,kernelSize),
                                  activation=KernalActivation,
                                  strides = c(strides,strides),
                                  padding = "valid")',d_)))
        eval(parse(text = sprintf('ConvProj%s = tf$keras$layers$Dense(nDimLowerDimConv,
                                      activation=KernalProjActivation)',d_)))
        eval(parse(text = sprintf('BNLayer_Axis3_%s <- tf$keras$layers$BatchNormalization(axis = 3L, center = T, scale = T, momentum = BN_MOMENTUM, epsilon = 0.001)',d_)))
        eval(parse(text = sprintf('BNLayer_Axis3_%s_inner <- tf$keras$layers$BatchNormalization(axis = 3L, center = T, scale = T, momentum = BN_MOMENTUM, epsilon = 0.001)',d_)))

        # every third, do pooling
        if(d_ %% poolingAt == 0){
          Pool = tf$keras$layers$MaxPool2D(pool_size = c(poolingBy,poolingBy))
          if(poolingType=="ave"){ Pool = tf$keras$layers$AveragePooling2D(pool_size = c(poolingBy,poolingBy)) }
        }
        if(d_ %% poolingAt != 0){ eval(parse(text = sprintf('Pool%s = tf$identity',d_))) }
      }
    }
    trainable_layers <- ls()[!ls() %in% c(trainable_layers)]

    HiddenDropout <- tf$keras$layers$Dropout(rate = dropoutRate )
    HiddenDropout_conv <- tf$keras$layers$Dropout(rate = dropoutRate )
    getProcessedImage <- tf_function_use( function(imm,training){
      # convolution + pooling
      for(d_ in 1:nDepthHidden_conv){
        if(doConvLowerDimProj){
          eval(parse(text = sprintf("imm <- BNLayer_Axis3_%s_inner(Pool( Conv%s( imm )),training=training)",d_,d_)))
          imm <- HiddenDropout_conv( imm ,training = training)
          if(d_ < nDepthHidden_conv){ eval(parse(text = sprintf("imm <- BNLayer_Axis3_%s(  ConvProj%s( imm ), training = training)",d_,d_))) }
        }
        if(doConvLowerDimProj == F){ eval(parse(text = sprintf("imm <- BNLayer_Axis3_%s( Pool( Conv%s( imm )), training = training)",d_,d_,d_)))}
      }
      return(imm)
    })

    getTreatProb <- tf_function_use( function(im_getProb, x_getProb, training_getProb){

      # flatten
      im_getProb <- GlobalPoolLayer( getProcessedImage(im_getProb, training = training_getProb) )
      im_getProb <- BNLayer_Axis1_inputDense(im_getProb, training = training_getProb)

      # concatinate with scene-level data
      im_getProb <- tf$concat(list(im_getProb,x_getProb),1L)

      # optimal hidden layer
      if(nDepthHidden_dense > 0){
        for(jrz in 1:nDepthHidden_dense){
        if(nDepthHidden_dense > 1 & jrz > 1){ im_getProb_m1 <- im_getProb }
        im_getProb <- eval(parse(text = sprintf("tf$keras$activations$swish( HiddenProjection%s(  im_getProb   ) )", jrz)))
        im_getProb <- eval(parse(text = sprintf("BNLayer_Axis1_hidden%s( im_getProb , training = training_getProb)",jrz)))
        if(nDepthHidden_dense > 1 & jrz > 1){ im_getProb <- tf$add(im_getProb, im_getProb_m1) }
        im_getProb <- HiddenDropout( im_getProb, training = training_getProb )
      } }

      # final projection layer + sigmoid
      im_getProb <- tf$keras$activations$sigmoid( DenseLayer( im_getProb   ) )

      # return
      return(  im_getProb  )
    })
    epsilonLabelSmooth <- tf$constant(0.01)
    getLoss <- tf_function_use( function(im_getLoss, x_getLoss, treatt_getLoss, mask, training_getLoss){
      treatProb <- getTreatProb( im_getProb = im_getLoss,
                                 x_getProb = x_getLoss,
                                 training_getProb = training_getLoss )
      treatt_r <- tf$cast(tf$reshape(treatt_getLoss,list(-1L,1L)),dtype=tf$float32)
      treatProb_r <- tf$reshape(treatProb,list(-1L,1L)) # check

      # final loss
      minThis <- tf$multiply(tf$math$log( tf$maximum(treatProb_r,0.001)),  (treatt_r)) +
                        tf$multiply(tf$math$log(tf$maximum(1-treatProb_r,0.001)),  (1-treatt_r))
      minThis <- tf$divide( tf$reduce_sum( tf$multiply(minThis, mask) ), tf$add(0.01, tf$reduce_sum(mask)))
      minThis <- tf$negative(  minThis  )
      return( minThis )
    })

    # arms
    print("Initializing training arm...")
    #for(ARM in c(T,F)){ # F arm is unnecessary, bloats memory
    for(ARM in c(T)){
      with(tf$GradientTape() %as% tape, {
        myLoss_forGrad <- getLoss( im_getLoss = InitImageProcess(im = ds_next_train[[1]],
                                                                 training = ARM,
                                                                 iteration = iterationFxn(1.)),
                                   x_getLoss = tf$constant( as.matrix(X[batch_indices,]),tf$float32),
                                   treatt_getLoss = tf$constant(as.matrix(obsW[batch_indices]),tf$float32 ),
                                   mask = tf$constant(as.matrix(1*(batch_indices %in% trainIndices)),tf$float32),
                                   training_getLoss = ARM )
      })
    }
    trainable_variables <- tape$watched_variables()

    # initialize beta
    init_beta_ref <- c(sapply(init_beta <- seq(-4,4,length.out = 1000),function(zer){ mean( 1/(1+exp(- (rnorm(1000) + zer))) )} ))
    init_beta <- init_beta [ which.min(abs(init_beta_ref - mean(obsW) ) ) ]
    if(samplingType == "initializeBeta"){
      print("INITIALIZING beta");BNLayer_Axis1_final$trainable_variables[[2]]$assign( tf$expand_dims(tf$constant(init_beta,dtype=tf$float32),0L) ) # beta is offset factor
    }

    # define optimizer and training step
    NA20 <- function(zer){zer[is.na(zer)] <- 0;zer[is.infinite(zer)] <- 0;zer}
    #optimizer_tf = tf$optimizers$Nadam() # pass
    optimizer_tf = tf$optimizers$legacy$Nadam()
    #optimizer_tf = tf$optimizers$legacy$Adam()
    getGrad <- tf_function_use(getGrad_r <- function(im_train, x_train, truth_train, mask){
      print("Initializing getGrad")
      with(tf$GradientTape(watch_accessed_variables=F) %as% tape, {
        tape$watch(  trainable_variables   )
        myLoss_forGrad <- getLoss( im_getLoss = im_train,
                                   x_getLoss = x_train,
                                   treatt_getLoss = truth_train,
                                   mask = mask,
                                   training_getLoss = T)
      })
      my_grads <- tape$gradient( myLoss_forGrad, trainable_variables )
      return(list(myLoss_forGrad, my_grads))
    })
    #trainStep <- tf_function_use(function(im_train, x_train, truth_train, mask){
    trainStep <- (function(im_train, x_train, truth_train, mask){
      #print("Initializing trainStep")
      # note: MEMORY LEAK WHEN TRAINSTEP IS TF_FUNCTION DECORATED
      my_grads <- getGrad(im_train, x_train, truth_train, mask)
      myLoss_forGrad <- my_grads[[1]]
      my_grads <- my_grads[[2]]
      optimizer_tf$apply_gradients(
                          rzip(my_grads, trainable_variables)[!unlist(lapply(my_grads,is.null)) ]
                    )
      #optimizer_tf$learning_rate$assign(   tf$constant(LEARNING_RATE_BASE*abs(cos(i/nSGD*widthCycle))*(i<nSGD/2)+ NA20(LEARNING_RATE_BASE*(i>=nSGD/2)/(i-nSGD/2+1)^.3) ) )
      return(  list( myLoss_forGrad, my_grads )  )
    })

    # number of trainable variables
    nTrainable <- sum( unlist(  lapply(trainable_variables,function(zer){ prod(dim(zer)) }) ) )
    print(sprintf("%s Trainable Parameters",nTrainable))

    if( typeBoot == "EstimationAndSampling" | typeBoot == "InitializationEstimationAndSampling" ){
        stop("That `typeBoot` option is under construction for the CNN arm!")
    }

    # perform training
    print("Starting training sequence...")
    loss_vec <- rep(NA, times = nSGD)
    in_ <- ip_ <- 0; for(i in 1:nSGD){
      if((i %% 10 == 0 | (i == 10) | i == nSGD) & doParallel == F | i < 50){
        print(sprintf("[%s] SGD Iteration: %i of %s", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), i, nSGD) );
        try(par(mfrow = c(1,1)),T);try(plot(loss_vec),T); try(points(smooth.spline(na.omit(loss_vec)),type="l",lwd=3),T)
      }
      if(i %% 1 == 0){ py_gc$collect() }

      # for debugging purposes
      # write.csv(file = sprintf("./checkpoint%s.csv",CommandArg_i), data.frame("CommandArg_i"=CommandArg_i, "i"=i))

      if(i == 1){ batch_indices_past <- myLoss_forGrad_past <- NA }
      if(i > 1){ myLoss_forGrad_past <- myLoss_forGrad; batch_indices_past <- batch_indices }

      if(acquireImageMethod == "functional"){
        if(samplingType != "balancedTrain"){
          batch_indices <- sample(trainIndices,batchSize,replace=F)
        }
        if(samplingType == "balancedTrain"){
          batch_indices <- c(sample(trainIndices[which(obsW[trainIndices]==1)], batchSize/2),
                             sample(trainIndices[which(obsW[trainIndices]==0)], batchSize/2) )
        }
        ds_next_train <- list(
          r2const( acquireImageFxn(imageKeysOfUnits[batch_indices], training = T) , dtype = tf$float32 )
        )
      }

      #if(i == 1){ds_next_train <- reticulate::iter_next( ds_iterator_train )}
      if(acquireImageMethod == "tf_record"){
        ds_next_train <- reticulate::iter_next( ds_iterator_train )

        # if we run out of observations, reset iterator...
        RestartedIterator <- F
        if( is.null(ds_next_train) ){
          print("Re-setting iterator! (type 1)")
          #tf$random$set_seed(as.integer(runif(1,1,1000000)))
          #tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
          #ds_next_train <- reticulate::iter_next( ds_iterator_train <- reticulate::as_iterator( tf_dataset_train ) ); RestartedIterator <- T
          ds_next_train <- reticulate::iter_next( ds_iterator_train )
        }

        if(!RestartedIterator){
          if(as.numeric(ds_next_train[[2]]$shape) != batchSize){
            # get a new batch if size mismatch - size mismatches generate new cached compiled fxns
            print("Re-setting iterator! (type 2)")
            #tf$random$set_seed(as.integer(runif(1,1,1000000)))
            #tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
            #ds_next_train <- reticulate::iter_next( ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )); RestartedIterator <- T

            ds_next_train <- reticulate::iter_next( ds_iterator_train )
          }
        }

        # save indices
        batch_indices <- c(as.array(ds_next_train[[2]]))
      }

      # get image and apply loss
      myLoss_forGrad <- trainStep(
        im_train = (ds_next_train[[1]] <-
                      InitImageProcess(ds_next_train[[1]],
                       training = T, iteration = iterationFxn(i))),
        x_train = tf$constant(as.matrix(X[batch_indices,]),dtype=tf$float32),
        truth_train = tf$constant(as.matrix(obsW[batch_indices]),tf$float32),
        mask = tf$constant(as.matrix(1*(batch_indices %in% trainIndices)),tf$float32)
      )
      optimizer_tf$learning_rate$assign(   tf$constant(LEARNING_RATE_BASE*abs(cos(i/nSGD*widthCycle))*(i<nSGD/2)+ NA20(LEARNING_RATE_BASE*(i>=nSGD/2)/(i-nSGD/2+1)^.3) ) )

      # post-processing checks
      loss_vec[i] <- as.numeric( myLoss_forGrad[[1]] )
      if(T == F){
        grad_norm <- f2n(try(sum(unlist(lapply(myLoss_forGrad[[2]],function(zer){sum(as.numeric(zer)^2)}))),T))
        if(is.na(loss_vec[i] ) | is.na(grad_norm) ){
          print("NA in loss -- opening browser")
          print("Image sum:")
          print(as.numeric(tf$math$reduce_sum(
              InitImageProcess(ds_next_train[[1]],
                training = T,
                iteration = iterationFxn(1.)
              ) )))
          print("Prior recent losses:")
          try( print(loss_vec[(i-10):i]), T)
          print("Keys:")
          print( ds_next_train[[1]] )
          print("Batch indices:")
          print( batch_indices )
          print("Table of batch sampled W's:" )
          print(table(  obsW[batch_indices] ))
          print("Sum of batch sampled X's:" )
          print(sum(  X[batch_indices,] ))
          browser()
          stop("NA introduced in training! Check images + input data for NAs. Try increasing batch size.")
        }
      }
    }
    print("Done with training sequence...")

    # remove big objects to free memory for inference
    rm(ds_next_train);rm(myLoss_forGrad)

    # get probabilities for inference (all observations)
    print("Starting to get probabilities for inference...")
    last_i <- 0; ok_counter <- 0; ok<-F;while(!ok){

      # for debugging purposes
      # write.csv(file = sprintf("./checkpoint2%s.csv",CommandArg_i), data.frame("CommandArg_i"=CommandArg_i, "ok_counter"=ok_counter))

      ok_counter <- ok_counter + 1
      gc();py_gc$collect()
      print(sprintf("[%s] %.3f%% done getting inference probabilities",
                    format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                    100*last_i / length(obsW)))

      # in functional mode
      if(acquireImageMethod == "functional"){
        batch_indices_inference <- (last_i+1):(last_i+batchSize)
        batch_indices_inference <- batch_indices_inference[batch_indices_inference<=length(obsW)]
        last_i <- batch_indices_inference[length(batch_indices_inference)]
        if(last_i == length(obsW)){ ok <- T }

        batchSizeOneCorrection <- F; if(length(batch_indices_inference) == 1){
          batch_indices_inference <- c(batch_indices_inference,batch_indices_inference)
          batchSizeOneCorrection <- T
        }

        batch_inference <- list(
          r2const( acquireImageFxn(imageKeysOfUnits[batch_indices_inference], training = F) , dtype = tf$float32 )
        )

        insert_probs <- try(c(as.array(getTreatProb(im_getProb = InitImageProcess(batch_inference[[1]],
                                                                                  training = F,
                                                                                  iteration = iterationFxn(1.)),
                                                    x_getProb = tf$constant(as.matrix(X[batch_indices_inference,]),dtype=tf$float32),
                                                    training_getProb = F ))),T)
        if( "try-error" %in% class(insert_probs)){
          print("Error in generating insert_probs in line 491! Investigating...")
          print("Output:")
          print( insert_probs )
          print("batch_inference[[1]]:")
          print(batch_inference[[1]])
          stop("Stopping due to try-error in insert_probs generation...")
        }
        if(batchSizeOneCorrection){ insert_probs <- insert_probs[-1]; batch_indices_inference <- batch_indices_inference[-1] }
        prW_est[batch_indices_inference] <- insert_probs
      }

      # in tf record mode
      if(acquireImageMethod == "tf_record"){
        batch_inference <- reticulate::iter_next( ds_iterator_inference )
        ok<-T;if(!all(is.null(batch_inference))){
          ok<-F
          batch_indices_inference <- as.array(batch_inference[[2]])
          drop_<-F;if(   length(batch_indices_inference) == 1    ){
            drop_ <- T
            batch_indices_inference <- c(batch_indices_inference, batch_indices_inference)
            batch_inference[[1]] <- tf$concat(list(batch_inference[[1]],batch_inference[[1]]),0L)
          }
          last_i <- max( batch_indices_inference )
          insert_probs <- try(c(as.array(getTreatProb(im_getProb = InitImageProcess(batch_inference[[1]],
                                                                                    training = F,
                                                                                    iteration = iterationFxn(1.)),
                                                      x_getProb = tf$constant(as.matrix(X[batch_indices_inference,]),dtype=tf$float32),
                                                      training_getProb = F ))),T)
          if(drop_ == T){  insert_probs <- insert_probs[-1]  }
          if("try-error"  %in% class(insert_probs)){browser()}
          prW_est[batch_indices_inference] <- insert_probs
        }
      }

      gc(); py_gc$collect()
    }
    rm( batch_inference )

    # clip extreme estimated probabilities
    prW_est[prW_est<0.01] <- 0.01
    prW_est[prW_est>0.99] <- 0.99
    print(   cor( c(obsW),c(prW_est) ) )
    if(any(is.na(prW_est)) ){
      print("Error: NAs in estimated probabilities! Reporting debugging information now...")
      print("Printing summary of probabilities...")
      print(summary( prW_est ) )
      print("Printing first probabilities...")
      print(head( prW_est ))
      print("Printing last probabilities...")
      print(tail( prW_est ))
      print("Printing NA indices...")
      print(which(is.na( prW_est  )))
      stop("Shutting down now due to NAs (see prior debugging messages)...")
    }

    tauHat_propensity <- mean(  obsW*obsY/(prW_est) - (1-obsW)*obsY/(1-prW_est) )
    tauHat_propensityHajek <- sum(  obsY*prop.table(obsW/(prW_est))) -
      sum(obsY*prop.table((1-obsW)/(1-prW_est) ))

    # sampling uncertainty only for cnn case
    tauHat_propensity_vec = sapply(1:nBoot,function(b_){
        ib_ <- sample(1:length(obsY), length(obsY), replace = T)
        tauHat_ <-  mean(  obsW[ib_]*obsY[ib_]/(prW_est[ib_]) -
                             (1-obsW[ib_])*obsY[ib_]/(1-prW_est[ib_]) )
      })
    tauHat_propensityHajek_vec <- sapply(1:nBoot,function(b_){
        ib_ <- sample(1:length(obsY), length(obsY), replace = T)
        tauHat_ <-  sum(  obsY[ib_]*prop.table(obsW[ib_]/(prW_est[ib_]))) -
          sum(obsY[ib_]*prop.table((1-obsW[ib_])/(1-prW_est[ib_]) ))
      })
    }

    if(modelClass == "embeddings"){
      if(acquireImageMethod == "tf_record"){ setwd(orig_wd)  }
      acquireImageFxnEmbeds <- NULL; if(!is.null(acquireImageFxn)){
        acquireImageFxn2 <- acquireImageFxn
        InitImageProcess2 <- InitImageProcess
        inputAvePoolingSize2 <- inputAvePoolingSize
        assign("InitImageProcess2", InitImageProcess2, envir = .GlobalEnv)
        assign("acquireImageFxn2", acquireImageFxn2, envir = .GlobalEnv)
        assign("inputAvePoolingSize2", inputAvePoolingSize2, envir = .GlobalEnv)
        acquireImageFxnEmbeds <- function(keys,
                                          acquireImageFxn_ = acquireImageFxn2,
                                          InitImageProcess_ = InitImageProcess2,
                                          training = F){
              InitImageProcess_( im = tf$constant(acquireImageFxn_( keys ),tf$float32),
                                 training = F,
                                 iteration = tf$constant(1.) )
        }
      }
      sigmoid<-function(.){1/(1+exp(-.))}
      tauHat_propensity_vec <- tauHat_propensityHajek_vec <- rep(NA,times = nBoot+1)
      for(jr in 1:(nBoot+1)){
        print( sprintf("Bootstrap iteration %s of %s [embeddings model class]", jr-1L, nBoot) )
        if(jr == 1){ indices_ <- 1:length( imageKeysOfUnits ) }
        if(jr > 1){ indices_ <- sample(1:length( imageKeysOfUnits ), length( imageKeysOfUnits ), replace = T) }

         # note: MyEmbeds_ are indexed by the original data ordering, resampling happens later
        if(typeBoot == "InitializationEstimationAndSampling" | jr == 1){
          MyEmbeds_ <- GetImageEmbeddings(
            imageKeysOfUnits = imageKeysOfUnits,
            batchSize = min(  batchSize, length(imageKeysOfUnits) ),
            acquireImageFxn = acquireImageFxnEmbeds,
            inputAvePoolingSize = inputAvePoolingSize,
            file = file,
            strides = strides,
            nEmbedDim = nEmbedDim,
            dataType = dataType,
            kernelSize = kernelSize,
            temporalKernelSize = temporalKernelSize,
            conda_env = "tensorflow_m1",
            conda_env_required = T
          )
        }

          # checks
          #tmp <- MyEmbeds$embeddings_fxn( acquireImageFxnEmbeds( imageKeysOfUnits ))

          # subset indices for training
          indices_forTraining <- indices_[indices_ %in% trainIndices]
          glmnetInput <- ifelse(XisNull,
                                yes = list(MyEmbeds_$embeddings),
                                no = list(cbind(as.matrix(X), MyEmbeds_$embeddings)))[[1]]
          myGlmnet_ <- glmnet::cv.glmnet(
                              x = as.matrix(glmnetInput[indices_forTraining,]),
                              y = as.matrix(obsW[indices_forTraining]),
                              alpha = 0, # alpha = 0 is the ridge penalty
                              family = "binomial")
          obsW_ <- obsW[indices_]
          obsY_ <- obsY[indices_]

          # compute QOIs
          myGlmnet_coefs_ <- as.matrix( glmnet::coef.glmnet(myGlmnet_, s = "lambda.min") )
          prW_est_ <- sigmoid( cbind(1, glmnetInput) %*% myGlmnet_coefs_ )
          tauHat_propensity_vec[jr] <- tauHat_propensity_ <- mean(  obsW_*obsY_/c(prW_est_) - (1-obsW_)*obsY_/c(1-prW_est_) )
          tauHat_propensityHajek_vec[jr] <- tauHat_propensityHajek_ <- sum(  obsY_*prop.table(obsW_/c(prW_est_))) -
            sum(obsY*prop.table((1-obsW_)/c(1-prW_est_) ))
          if(jr == 1){
            nTrainable <- length(  myGlmnet_coefs_  )
            tauHat_propensityHajek <- tauHat_propensityHajek_
            tauHat_propensity <- tauHat_propensity_
            myGlmnet_coefs <- myGlmnet_coefs_
            myGlmnet_coefs_mat <- matrix(NA, nrow = nBoot+1,
                                         ncol = length(myGlmnet_coefs_))
            prW_est <- prW_est_
            embeddings_fxn <- MyEmbeds_$embeddings_fxn

            myGlmnet_coefs_tf <- tf$constant(myGlmnet_coefs, dtype = tf$float32)
            getTreatProb <- tf_function_use( function(im_getProb, x_getProb, training_getProb){
              if(!XisNull){
                concatDat <- tf$concat(list(
                               tf$ones(list(im_getProb$shape[[1]],1L)),
                                     x_getProb, embeddings_fxn( im_getProb )), 1L)
              }
              if(XisNull){
                concatDat <- tf$concat(list(
                  tf$ones(list(im_getProb$shape[[1]],1L)), embeddings_fxn( im_getProb )
                ), 1L) }
              my_probs <- tf$nn$sigmoid(  tf$matmul(concatDat, myGlmnet_coefs_tf)  )
            })
          }
          myGlmnet_coefs_mat[jr,] <- c(myGlmnet_coefs_)
      }
      if(acquireImageMethod == "tf_record"){ setwd(new_wd)  }
    }

    # process in and out of sample losses
    prWEst_baseline <- prW_est
    prWEst_baseline[] <- mean( obsW[trainIndices] )
    lossCE_OUT_baseline <- binaryCrossLoss(obsW[testIndices], prWEst_baseline[testIndices])
    lossCE_IN_baseline <- binaryCrossLoss(obsW[trainIndices], prWEst_baseline[trainIndices])
    lossCE_OUT <-  binaryCrossLoss(  obsW[testIndices], prW_est[testIndices]  )
    lossCE_IN <-  binaryCrossLoss(  obsW[trainIndices], prW_est[trainIndices]  )
    lossClassError_OUT_baseline <- 1/length(testIndices) * (sum( prWEst_baseline[testIndices][ obsW[testIndices] == 1] < 0.5) +
                           sum( prWEst_baseline[testIndices][ obsW[testIndices] == 0] > 0.5))
    lossClassError_IN_baseline <- 1/length(trainIndices) * (sum( prWEst_baseline[trainIndices][ obsW[trainIndices] == 1] < 0.5) +
                                                              sum( prWEst_baseline[trainIndices][ obsW[trainIndices] == 0] > 0.5))
    lossClassError_OUT <- 1/length(testIndices) * (sum( prW_est[testIndices][ obsW[testIndices] == 1] < 0.5) +
                           sum( prW_est[testIndices][ obsW[testIndices] == 0] > 0.5))
    lossClassError_IN <- 1/length(trainIndices) * (sum( prW_est[trainIndices][ obsW[trainIndices] == 1] < 0.5) +
                                                     sum( prW_est[trainIndices][ obsW[trainIndices] == 0] > 0.5))
    ModelEvaluationMetrics <- list(
      "CELoss_out" = lossCE_OUT,
      "CELoss_out_baseline" = lossCE_OUT_baseline,
      "CELoss_in" = lossCE_IN,
      "CELoss_in_baseline" = lossCE_IN_baseline,
      "ClassError_out" = lossClassError_OUT,
      "ClassError_out_baseline" = lossClassError_OUT_baseline,
      "ClassError_in" = lossClassError_IN,
      "ClassError_in_baseline" = lossClassError_IN_baseline
    )

    # reset to original wd which was altered during records initialization
    # do this before plotting to avoid disrupting plot save locations
    if( changed_wd ){ setwd(  orig_wd  ) }

    # do some analysis with examples
    processedDims <- NULL
    if(    plotResults == T  ){
      print("Starting to plot the image confounding results...")
      # get treatment image
      indices_t <- (1:length(obsW))[which(obsW==1)]
      indices_c <- (1:length(obsW))[which(obsW==0)]

      showPerGroup <- min(c(3,unlist(table(obsW))), na.rm = T)
      ordered_control <- indices_c[order_c <- order(prW_est[indices_c],decreasing = F)]
      ordered_treated <- indices_t[order_t <- order(prW_est[indices_t],decreasing = T)]
      # checks
      # prW_est[ ordered_treated ];
      # prW_est[ ordered_control ]

      # drop duplicates
      top_treated <- ordered_treated
      top_control <- ordered_control
      if(!is.null(long)){
        longLat_t <- paste(round(long[indices_t[order_t]],5L),
                                round(lat[indices_t[order_t]],5L),sep="_")
        longLat_c <- paste(round(long[indices_c[order_c]],5L),
                                round(lat[indices_c[order_c]],5L),sep="_")
        top_treated <- ordered_treated[!duplicated(longLat_t)]
        top_control <- ordered_control[!duplicated(longLat_c)]
      }
      top_treated <- top_treated[1:showPerGroup]
      top_control <- top_control[1:showPerGroup]

      # checks
      # prW_est[ top_treated ]; obsW[ top_treated ];
      # prW_est[ top_control ]; obsW[ top_control ];

      # concatenate c and t indices
      plot_indices <- c(top_control, top_treated)
      makePlots <- function(){
        print("Starting to make output plots...")

        try({
        print("Plotting salience maps...")
        nrows_im <- (modelClass=="cnn")*3 + (modelClass=="embeddings")*2
        pdf(sprintf("%s/CSM_KW%s_InputAvePool%s_%s_Tag%s.pdf",
                    figuresPath,
                    kernelSize,
                    inputAvePoolingSize,
                    modelClass,
                    figuresTag),
            width = length(plot_indices)*5+2,height = nrows_im*5)
        {
          layout(matrix(1:(nrows_im*(1+length(plot_indices))),
                        ncol = 1+length(plot_indices)),
                 width = c(0.5,rep(5,length(plot_indices))),
                 height = rep(5,times=nrows_im)); in_counter <- 0
          for(text_ in c("Raw Image","Salience Map","Final Spatial Layer")[1:nrows_im]){
            par(mar=c(0,0,0,0))
            plot(0, main = "", ylab = "",cex = 0,
                 xlab = "", ylim = c(0,1), xlim = c(0,1),
                 xaxt = "n",yaxt = "n",bty = "n")
            text(0.5,0.5,labels = text_, srt=90,cex=3)
          }
          plot_index_counter <- 0; for(in_ in plot_indices){
            gc(); py_gc$collect()
            plot_index_counter <- plot_index_counter + 1
            if(acquireImageMethod == "tf_record"){
              #setwd(orig_wd)
              ds_next_in <- GetElementFromTfRecordAtIndices( indices = in_,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             nObs = length(imageKeysOfUnits) )
              #setwd(new_wd)
              if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
            }
            if(acquireImageMethod == "functional"){
              ds_next_in <- r2const( acquireImageFxn(imageKeysOfUnits[in_], training = F), dtype = tf$float32 )
              if(length(ds_next_in$shape) == 3){ ds_next_in <- tf$expand_dims(ds_next_in,0L) }
              ds_next_in <- list( ds_next_in )
            }

            #print(in_)
            col_ <- ifelse(in_ %in% top_treated, yes = "black", no = "gray")
            in_counter <- in_counter + 1
            if(  !is.null(lat)  ){
              long_lat_in_ <- sprintf("Lat-Lon: %.3f, %.3f", f2n(lat[in_]), f2n(long[in_]))
            }

            # extract
            im_orig <- im_ <- InitImageProcess(
                                im = ds_next_in[[1]], training = F,
                                iteration = tf$constant(1.))
            XToConcat_values <- tf$constant(t(X[in_,]),tf$float32)
            if(modelClass == "cnn"){
              im_processed <- getProcessedImage(im_, training = F)
              processedDims <- dim(im_)
            }
            im_ <- as.array(tf$squeeze(im_,c(0L)))

            # calculate salience map using log probabilities
            salience_map_calc <- function(){
              with(tf$GradientTape() %as% tape, {
                tape$watch(im_orig)
                treat_prob_im <- tf$squeeze(tf$squeeze(getTreatProb( im_getProb = im_orig,
                                                                     x_getProb = XToConcat_values,
                                                                     training_getProb = F),0L),0L)
                treat_prob_im <- tf$math$log(tf$add(0.001,treat_prob_im))
              })

              salience_map <- tape$gradient( treat_prob_im, im_orig )
            }
            salience_map <- salience_map_calc()
            salience_map <- tf$math$reduce_euclidean_norm(salience_map,3L,keepdims=T)
            salience_map <- tf$keras$layers$AveragePooling2D(c(7L,7L))(salience_map)
            salience_map <- as.array(salience_map)[1,,,]
            salience_map <- apply(salience_map^2,1:2,sum)^0.5

            # do plotting
            orig_scale_im_ <- sapply(1:length(NORM_MEAN),
                                     function(band_){
                                       im_[,,band_] <- 0.1+im_[,,band_]*NORM_SD[band_] + NORM_MEAN[band_]
                                       im_[,,band_] }, simplify="array")
            par(mar = (mar_vec <- c(2,1,3,1)))

            # plot raw image
            if(length(plotBands) < 3){
              causalimages::image2(
                as.matrix( orig_scale_im_[,,plotBands[1]] ),
                main = long_lat_in_, cex.main = 2.5, col.main =  col_,
                xlab = ifelse( plot_index_counter == 1,
                               yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                               no = "")
              )
            }
            if(length(plotBands) >= 3){
               plot(0, main = long_lat_in_, col.main = col_,
                    ylab = "", xlab = "",
                    cex.main = 4, ylim = c(0,1), xlim = c(0,1),
                    cex = 0, xaxt = "n",yaxt = "n",bty = "n")
               mtext(side = 1, ifelse( plot_index_counter == 1,
                             yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                             no = ""), cex = 1)
               orig_scale_im_raster <- raster::brick(orig_scale_im_[,,plotBands[1:3]])
               try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                               add = T, main = long_lat_in_, stretch = "lin"), T)
               if("try-error" %in% class(try_)){
                 try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                             add = T, main = long_lat_in_), T)
               }
            }

            # plot salience map
            par(mar = mar_vec)
            salience_map[salience_map>0] <- salience_map[salience_map>0] / (0.001+sd(salience_map[salience_map>0]))
            # print(summary(c(salience_map)))
            salience_map <- sign(salience_map)*log(abs(salience_map)+1)
            if(nrows_im == 2){
              mar_vec_finalIm <- mar_vec
              mar_vec_finalIm[1] <- 4
              par(mar = mar_vec_finalIm)
            }
              image2( salience_map,
                                    xlab = ifelse(tagInFigures & nrows_im == 2,
                                                yes = imageKeysOfUnits[in_], no = ""),cex.lab = 1)
            if(nrows_im == 2){ par(mar = mar_vec) }

            # plot final layer
            if(nrows_im > 2){
              mar_vec_finalIm <- mar_vec
              mar_vec_finalIm[1] <- 4
              par(mar = mar_vec_finalIm)
              image2( as.array(im_processed)[1,,,1],
                      xlab = ifelse(tagInFigures, yes = imageKeysOfUnits[in_], no = ""),cex.lab = 1)
              par(mar = mar_vec)
            }
          }
        }
        dev.off()
        }, T)

        if(modelClass == "cnn"){
          pdf(sprintf("%s/Loss_KW%s_InputAvePool%s_%s_Tag%s.pdf",
                      figuresPath,
                      kernelSize,
                      inputAvePoolingSize,
                      modelClass,
                      figuresTag))
            par(mar = c(5,5,1,1))
            try(plot(loss_vec, cex = 1.5, cex.lab = 2,
                     xlab = "Iteration",
                     ylab = "Loss"),T);
            try(points(smooth.spline(na.omit(loss_vec)),type="l",lwd=3),T)
          dev.off()
        }

        try({
        print("Plotting propensity histogram...")
        pdf(sprintf("%s/Hist_KW%s_InputAvePool%s_%s_Tag%s.pdf",
                    figuresPath,
                    kernelSize,
                    inputAvePoolingSize,
                    modelClass,
                    figuresTag))
        {
          par(mfrow=c(1,1))
          d0 <- density(prW_est[obsW==0])
          d1 <- density(prW_est[obsW==1])
          plot(d1,lwd=2,xlim = c(-0.1,1.1),ylim =c(0,max(c(d1$y,d0$y),na.rm=T)*1.2),
               cex.axis = 1.2,ylab = "",xaxt = "n",
               xlab = ifelse(tagInFigures, yes = figuresTag, no = ""),
               main = "Density Plots for \n Estimated Pr(T=1 | Confounders)",cex.main = 2)
          axis(1, at = seq(0,1,by = 0.25))
          points(d0,lwd=2,type = "l",col="gray",lty=2)
          text(d0$x[which.max(d0$y)[1]],
               max(d0$y,na.rm=T)*1.1,label = "T = 0",col="gray",cex=2)
          text(d1$x[which.max(d1$y)[1]],
               max(d1$y,na.rm=T)*1.1,label = "T = 1",col="black",cex=2)
        }
        dev.off()
        }, T)
      }

      #try(makePlots(),T)
      makePlots()
    }

    # compute salience for tabular covariates
    SalienceX_se <- SalienceX <- NULL; if(!XisNull){
    if(modelClass == "cnn"){
        getSalienceVec <- function(im_, x_){
          x_ <- tf$Variable(x_,trainable = T)
          with(tf$GradientTape() %as% tape, {
            tape$watch(x_)
            treat_prob_im <- tf$squeeze(tf$squeeze(getTreatProb( im_getProb = im_,
                                                                 x_getProb = x_,
                                                                 training_getProb = F),0L),0L)

            # calc salience using log probabilities
            treat_prob_im <- tf$math$log( treat_prob_im )
          })
          return(  salience_vec <- tape$gradient( treat_prob_im, x_ )   ) }
        SalienceX <- c(); samp_counter <- 0
        for(samp_ in sample(1:nrow(X),100,replace = T)){
          print(sprintf("Tabular Salience Iteration %s of %s", samp_counter <- samp_counter + 1, 100))
          if(acquireImageMethod == "tf_record"){
            #setwd(orig_wd)
            ds_next_in <- GetElementFromTfRecordAtIndices( indices = samp_,
                                                           filename = file,
                                                           readVideo = useVideo,
                                                           nObs = length(imageKeysOfUnits) )
            #setwd(new_wd)
            if(length(ds_next_in$shape) == 3){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          }
          if(acquireImageMethod == "functional"){
            ds_next_in <- r2const( acquireImageFxn(imageKeysOfUnits[ samp_ ], training = F), dtype = tf$float32 )
            if(length(ds_next_in$shape) == 3){ ds_next_in <- tf$expand_dims(ds_next_in,0L) }
            ds_next_in <- list( ds_next_in )
          }

        im_ <- InitImageProcess(
                          ds_next_in[[1]],
                          training = F,
                          iteration = tf$constant(1.))
        x_ <- tf$constant(t(X[samp_,]),tf$float32)
        SalienceX <- rbind(SalienceX,as.matrix( getSalienceVec(im_=im_, x_=x_)))
        }
        SalienceX <- colMeans( SalienceX ); names( SalienceX ) <- colnames(X)

        # rescale the salience map into original scale
        #SalienceX <- SalienceX*X_sd  +   X_mean
      }
      if(modelClass != "cnn"){
        SalienceX <- myGlmnet_coefs[-1][1:ncol(X)] # drop intercept, then extract variables of interest
        SalienceX_se <- apply(myGlmnet_coefs_mat, 2, sd)[-1][1:ncol(X)]
        if(!is.null(SalienceX)){ names(SalienceX_se) <- colnames(X) }
    } }

    postDiffInLat <- preDiffInLat <- NULL
    if(!is.null(lat)){
      preDiffInLat <- colMeans(cbind(long[obsW == 1],lat[obsW == 1])) -
        colMeans(cbind(long[obsW == 0],lat[obsW == 0]))
      postDiffInLat <- colSums(cbind(long[obsW == 1],lat[obsW == 1])*prop.table(1/prW_est[obsW == 1])) -
        colSums(cbind(long[obsW == 0],lat[obsW == 0])*prop.table(1/(1-prW_est[obsW == 0])))
    }

    # set salience map names
    if(!is.null(SalienceX)){ names(SalienceX) <- colnames(X) }

    print(  "Done with image confounding analysis!"  )
    se <- function(x){ x <- c(na.omit(x)); return(sqrt(var(x)/length(x)))}
    return(    list(
      "tauHat_propensityHajek"  = tauHat_propensityHajek,
      "tauHat_propensityHajek_se"  = sd(tauHat_propensityHajek_vec,na.rm=T),
      "tauHat_diffInMeans"  = mean(obsY[which(obsW==1)],na.rm=T) - mean(obsY[which(obsW==0)],na.rm=T),
      "tauHat_diffInMeans_se"  = c(sqrt(se(obsY[which(obsW==1)])^2 + se(obsY[which(obsW==0)])^2)),
      "SalienceX" = SalienceX,
      "SalienceX_se" = SalienceX_se,
      "prW_est" = prW_est,
      "SGD_loss_vec" = loss_vec,
      "LatitudeAnalysis" = list("preDiffInLat" = preDiffInLat,
                                "postDiffInLat"  = postDiffInLat),
      "ModelEvaluationMetrics" = ModelEvaluationMetrics,
      "nTrainableParameters" = nTrainable
    ) )
  }
}
